{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ac2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5815f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # load input file content: one dino name per line\n",
    "        file_content = open(\"./data/dinos.txt\").read().lower()\n",
    "        self.data = file_content.split(\"\\n\")\n",
    "        \n",
    "        # note: build chars from file_content so that it includes \"\\n\"\n",
    "        self.chars = sorted(set(file_content))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # to create a OneHotEncoder manually\n",
    "        self.char_to_ix = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.ix_to_char = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # every training example `name` consists of `len_name` + 1\n",
    "        # input characters and `len_name` + 1 output characters\n",
    "        name = self.data[index]\n",
    "        len_name = len(name)\n",
    "        \n",
    "        # input X is $x_0=\\vec{0}$ and the chars from `name`\n",
    "        X = torch.zeros((1 + len_name, self.vocab_size))\n",
    "        for ii, char in enumerate(list(name)):\n",
    "            X[ii + 1, self.char_to_ix[char]] = 1\n",
    "        \n",
    "        # output Y is the 1-shifted chars from `name` and a final \"\\n\"\n",
    "        Y = torch.zeros((len_name + 1, self.vocab_size))\n",
    "        Y[:-1, :] = X[1:, :]\n",
    "        Y[-1, self.char_to_ix[\"\\n\"]] = 1 \n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c38cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "        An RNN model that consists of\n",
    "        1 RNN layer, followed by 1 linear layer\n",
    "        (assumes batch_first = True in x)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_size, h_size, out_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.hidden_size = h_size\n",
    "        \n",
    "        # parameters\n",
    "        self.rnn = nn.RNN(in_size, h_size, batch_first=True)\n",
    "        self.linear = nn.Linear(h_size, out_size)\n",
    "        \n",
    "    def _init_hidden(self, batch_size):\n",
    "        \"\"\"initialize hidden cell states\"\"\"\n",
    "        return torch.zeros((batch_size, 1, self.hidden_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        #print(x.shape, hidden.shape)\n",
    "        out_h, _ = self.rnn(x, hidden)\n",
    "        out_l = self.linear(out_h.view(-1, self.hidden_size))\n",
    "        return out_l\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577b0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    return txt\n",
    "\n",
    "def sample(model, seed, max_length=50):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "        model \n",
    "        seed\n",
    "        max_length -- maximum number of characters generated for each name\n",
    "    \n",
    "    Returns:\n",
    "        indices -- A list of the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    vocab_size = len(model.state_dict()[\"linear.bias\"])\n",
    "    n_a = model.state_dict()[\"linear.weight\"].shape[1]\n",
    "    \n",
    "    # Step 1: Create the a zero vector x that can be used as the one-hot vector \n",
    "    # Representing the first character (initializing the sequence generation). (≈1 line) \n",
    "    x = torch.zeros((1, 1, vocab_size))\n",
    "    \n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = torch.zeros((n_a, 1))\n",
    "    \n",
    "    # Create an empty list of indices. This is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # idx is the index of the one-hot vector x that is set to 1\n",
    "    # All other positions in x are zero.\n",
    "    # Initialize idx to -1\n",
    "    idx = -1\n",
    "    \n",
    "    # Loop over time-steps t. At each time-step:\n",
    "    # Sample a character from a probability distribution \n",
    "    # And append its index (`idx`) to the list \"indices\".\n",
    "    # Stop if you reach a new line character or `max_length` characters \n",
    "    counter = 0\n",
    "    newline_char_idx = train_data.char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_char_idx and counter != max_length):\n",
    "        \n",
    "        # Step 2: Forward propagate x\n",
    "        yhat = model(x)\n",
    "        probs = torch.softmax(yhat, dim=1).detach().numpy().ravel()\n",
    "        \n",
    "        # for grading purposes\n",
    "        np.random.seed(counter + seed) \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary \n",
    "        # from the probability distribution \n",
    "        idx = np.random.choice(range(train_data.vocab_size), p=probs)\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input x with one \n",
    "        # that corresponds to the sampled index `idx`.\n",
    "        x = torch.zeros((1, 1, vocab_size))\n",
    "        x[0, 0, idx] = 1\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter += 1\n",
    "    \n",
    "    if (counter == max_length):\n",
    "        indices.append(newline_char_idx)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d8c1de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train()\n",
    "def optimize(model, train_loader, num_epochs, \n",
    "             loss_fn, optimizer, \n",
    "             clip_value=5, num_dino_names_to_sample=7):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.\n",
    "        \n",
    "        for X, Y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward propagate through time\n",
    "            Yhat = model(X)\n",
    "            loss = loss_fn(Yhat, Y[0])\n",
    "            \n",
    "            # backpropagate through time\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip\n",
    "            nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "            \n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # training loss after each iteration\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # training loss after each epoch\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1} / {num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # validate\n",
    "        if epoch % 10 == 0:\n",
    "            seed = 0\n",
    "            for name in range(num_dino_names_to_sample):\n",
    "                sampled_indices = sample(model, seed)\n",
    "                last_dino_name = get_sample(sampled_indices, train_data.ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44fc7356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "train_data = DinoDataset()\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "#test_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "# model\n",
    "in_size = train_data.vocab_size\n",
    "h_size = 50\n",
    "model = RNN(in_size, h_size, in_size)\n",
    "\n",
    "# loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe3c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 1000], Loss: 2.8055\n",
      "Njyvusandpodygrpuasijius\n",
      "Jnda\n",
      "Kyvusandpodygrpuasijius\n",
      "Nda\n",
      "Yvusandpodygrpuasijius\n",
      "Ea\n",
      "Vusandpodygrpuasijius\n",
      "\n",
      "\n",
      "Epoch [2 / 1000], Loss: 2.4221\n",
      "Epoch [3 / 1000], Loss: 2.2258\n",
      "Epoch [4 / 1000], Loss: 2.1346\n",
      "Epoch [5 / 1000], Loss: 2.0741\n",
      "Epoch [6 / 1000], Loss: 2.0274\n",
      "Epoch [7 / 1000], Loss: 1.9937\n",
      "Epoch [8 / 1000], Loss: 1.9662\n",
      "Epoch [9 / 1000], Loss: 1.9441\n",
      "Epoch [10 / 1000], Loss: 1.9241\n",
      "Epoch [11 / 1000], Loss: 1.9070\n",
      "Mgytspcidoraweshtasamespantfzhbamakvafecihafpctokp\n",
      "Jlba\n",
      "Kytspcidoraweshtasamespantfzhbamakvafecihafpctokpn\n",
      "Macadrtdacitgtddivlcbzalosacusdcjdbopthceqpmapdsjl\n",
      "Ytspcidoraweshtasamespantfzhbamakvafecihafpctokpnm\n",
      "Ca\n",
      "Tspcidoraweshtasamespantfzhbamakvafecihafpctokpnma\n",
      "\n",
      "\n",
      "Epoch [12 / 1000], Loss: 1.8911\n",
      "Epoch [13 / 1000], Loss: 1.8767\n",
      "Epoch [14 / 1000], Loss: 1.8635\n",
      "Epoch [15 / 1000], Loss: 1.8513\n",
      "Epoch [16 / 1000], Loss: 1.8400\n",
      "Epoch [17 / 1000], Loss: 1.8297\n",
      "Epoch [18 / 1000], Loss: 1.8213\n",
      "Epoch [19 / 1000], Loss: 1.8110\n",
      "Epoch [20 / 1000], Loss: 1.8022\n",
      "Epoch [21 / 1000], Loss: 1.7950\n",
      "Mewtrodnerlcusprtasamdrpansbymbamakukbechabbodspdo\n",
      "Ipecadqtedantgtelithacychktactopepgchhusaknmgdogsm\n",
      "Jytrodnerlcusprtasamdrpansbymbamakukbechabbodspdop\n",
      "Macadqtedantgtelithacychktactopepgchhusaknmgdogsmg\n",
      "Ytrodnerlcusprtasamdrpansbymbamakukbechabbodspdopp\n",
      "Da\n",
      "Trodnerlcusprtasamdrpansbymbamakukbechabbodspdoppc\n",
      "\n",
      "\n",
      "Epoch [22 / 1000], Loss: 1.7860\n",
      "Epoch [23 / 1000], Loss: 1.7786\n",
      "Epoch [24 / 1000], Loss: 1.7725\n",
      "Epoch [25 / 1000], Loss: 1.7647\n",
      "Epoch [26 / 1000], Loss: 1.7598\n",
      "Epoch [27 / 1000], Loss: 1.7530\n",
      "Epoch [28 / 1000], Loss: 1.7475\n",
      "Epoch [29 / 1000], Loss: 1.7409\n",
      "Epoch [30 / 1000], Loss: 1.7340\n",
      "Epoch [31 / 1000], Loss: 1.7297\n",
      "Mevtptanerkbwbpstasbemptanscwchamakulchangafpcosas\n",
      "Inedadrsacantesakhusacychisacrpodolbmhusakmkidodrn\n",
      "Jytptanerkbwbpstasbemptanscwchamakulchangafpcosash\n",
      "Macadrsacantesakhusacychisacrpodolbmhusakmkidodrng\n",
      "Xtptanerkbwbpstasbemptanscwchamakulchangafpcosashe\n",
      "Cbadrsacantesakhusacychisacrpodolbmhusakmkidodrngl\n",
      "Tptanerkbwbpstasbemptanscwchamakulchangafpcosasheg\n",
      "\n",
      "\n",
      "Epoch [32 / 1000], Loss: 1.7257\n",
      "Epoch [33 / 1000], Loss: 1.7195\n",
      "Epoch [34 / 1000], Loss: 1.7148\n",
      "Epoch [35 / 1000], Loss: 1.7105\n",
      "Epoch [36 / 1000], Loss: 1.7051\n",
      "Epoch [37 / 1000], Loss: 1.7008\n",
      "Epoch [38 / 1000], Loss: 1.6968\n",
      "Epoch [39 / 1000], Loss: 1.6923\n",
      "Epoch [40 / 1000], Loss: 1.6889\n",
      "Epoch [41 / 1000], Loss: 1.6837\n",
      "Mdystododosavesptarbemptanphymabeggulchangadodrodr\n",
      "Infa\n",
      "Jystododosavesptarbemptanphymabeggulchangadodrodrh\n",
      "Macadrracansdraglvhacxalosacrppgngdoquscherfchaspm\n",
      "Xtosanfpravesptarbemptanphymabeggulchangadodrodrhe\n",
      "Cbadrracansdraglvhacxalosacrppgngdoquscherfchaspmd\n",
      "Tosanfpravesptarbemptanphymabeggulchangadodrodrheg\n",
      "\n",
      "\n",
      "Epoch [42 / 1000], Loss: 1.6812\n",
      "Epoch [43 / 1000], Loss: 1.6758\n",
      "Epoch [44 / 1000], Loss: 1.6729\n",
      "Epoch [45 / 1000], Loss: 1.6686\n",
      "Epoch [46 / 1000], Loss: 1.6667\n",
      "Epoch [47 / 1000], Loss: 1.6623\n",
      "Epoch [48 / 1000], Loss: 1.6591\n",
      "Epoch [49 / 1000], Loss: 1.6556\n",
      "Epoch [50 / 1000], Loss: 1.6521\n",
      "Epoch [51 / 1000], Loss: 1.6494\n",
      "Mevrstangoravesptaphamptanphymabeggumalangadodrnfq\n",
      "Ingbadrracanshschavelaychisacpthangdopthagninerasp\n",
      "Jystododoravesptaphamptanphymabeggumalangadodrnfqn\n",
      "Macadrracanshschavelaychisacpthangdopthagnineraspn\n",
      "Xtosangoravesptaphamptanphymabeggumalangadodrnfqng\n",
      "Cebastacanshschavelaychisacpthangdopthagnineraspng\n",
      "Tosangoravesptaphamptanphymabeggumalangadodrnfqngc\n",
      "\n",
      "\n",
      "Epoch [52 / 1000], Loss: 1.6449\n",
      "Epoch [53 / 1000], Loss: 1.6416\n",
      "Epoch [54 / 1000], Loss: 1.6398\n",
      "Epoch [55 / 1000], Loss: 1.6357\n",
      "Epoch [56 / 1000], Loss: 1.6333\n",
      "Epoch [57 / 1000], Loss: 1.6298\n",
      "Epoch [58 / 1000], Loss: 1.6276\n",
      "Epoch [59 / 1000], Loss: 1.6244\n",
      "Epoch [60 / 1000], Loss: 1.6214\n",
      "Epoch [61 / 1000], Loss: 1.6197\n",
      "Mevrstangngfxdrktaphamptanphymabeggumalangadodrngp\n",
      "Helabastacbgsjtagmusbaychisacosphelblmtfhangmangsp\n",
      "Itostangngfxdrktaphamptanphymabeggumalangadodrngpp\n",
      "Macadrqedansjtagmusbaychisacosphelblmtfhangmangspo\n",
      "Xtosangngfxdrktaphamptanphymabeggumalangadodrngpps\n",
      "Cebastacbgsjtagmusbaychisacosphelblmtfhangmangspom\n",
      "Tosangngfxdrktaphamptanphymabeggumalangadodrngppsa\n",
      "\n",
      "\n",
      "Epoch [62 / 1000], Loss: 1.6152\n",
      "Epoch [63 / 1000], Loss: 1.6135\n",
      "Epoch [64 / 1000], Loss: 1.6108\n",
      "Epoch [65 / 1000], Loss: 1.6089\n",
      "Epoch [66 / 1000], Loss: 1.6061\n",
      "Epoch [67 / 1000], Loss: 1.6036\n",
      "Epoch [68 / 1000], Loss: 1.6013\n",
      "Epoch [69 / 1000], Loss: 1.5990\n",
      "Epoch [70 / 1000], Loss: 1.5968\n",
      "Epoch [71 / 1000], Loss: 1.5942\n",
      "Mctostangnggwapstaphamptanolymabceltadiangadodrmap\n",
      "Hchabaqtacanshschavelaxalosacosphbadonvemangngngrk\n",
      "Itostangnggwapstaphamptanolymabceltadiangadodrmapo\n",
      "Macadrphachshschavelaxalosacosphbadonvemangngngrkj\n",
      "Xtosangnggwapstaphamptanolymabceltadiangadodrmapop\n",
      "Cebaqtacanshschavelaxalosacosphbadonvemangngngrkje\n",
      "Tosangnggwapstaphamptanolymabceltadiangadodrmapope\n",
      "\n",
      "\n",
      "Epoch [72 / 1000], Loss: 1.5924\n",
      "Epoch [73 / 1000], Loss: 1.5892\n",
      "Epoch [74 / 1000], Loss: 1.5872\n",
      "Epoch [75 / 1000], Loss: 1.5852\n",
      "Epoch [76 / 1000], Loss: 1.5836\n",
      "Epoch [77 / 1000], Loss: 1.5809\n",
      "Epoch [78 / 1000], Loss: 1.5785\n",
      "Epoch [79 / 1000], Loss: 1.5774\n",
      "Epoch [80 / 1000], Loss: 1.5741\n",
      "Epoch [81 / 1000], Loss: 1.5720\n",
      "Mevostangnggwapstaphamptanokymabejauralangbandrngo\n",
      "Ingcadrphachustagmthactakosacqthanggngtengngngngrm\n",
      "Jytosangnggwapstaphamptanokymabejauralangbandrngon\n",
      "Macadrphachustagmthactakosacqthanggngtengngngngrmg\n",
      "Xtosangnggwapstaphamptanokymabejauralangbandrngong\n",
      "Dcadrphachustagmthactakosacqthanggngtengngngngrmgm\n",
      "Tosangnggwapstaphamptanokymabejauralangbandrngongd\n",
      "\n",
      "\n",
      "Epoch [82 / 1000], Loss: 1.5710\n",
      "Epoch [83 / 1000], Loss: 1.5689\n",
      "Epoch [84 / 1000], Loss: 1.5658\n",
      "Epoch [85 / 1000], Loss: 1.5658\n",
      "Epoch [86 / 1000], Loss: 1.5629\n",
      "Epoch [87 / 1000], Loss: 1.5614\n",
      "Epoch [88 / 1000], Loss: 1.5590\n",
      "Epoch [89 / 1000], Loss: 1.5579\n",
      "Epoch [90 / 1000], Loss: 1.5559\n",
      "Epoch [91 / 1000], Loss: 1.5522\n",
      "Mdytosangnggwapstaphamptanodymabceltadochachhathap\n",
      "Helabastacbeskschavelaxajksacpthanggngtengngngngrl\n",
      "Itostangnggwapstaphamptanodymabceltadochachhathapr\n",
      "Macadrodedoskschavelaxajksacpthanggngtengngngngrli\n",
      "Xtosangnggwapstaphamptanodymabceltadochachhathaprg\n",
      "Cebastacbeskschavelaxajksacpthanggngtengngngngrlim\n",
      "Tosangnggwapstaphamptanodymabceltadochachhathaprge\n",
      "\n",
      "\n",
      "Epoch [92 / 1000], Loss: 1.5520\n",
      "Epoch [93 / 1000], Loss: 1.5493\n",
      "Epoch [94 / 1000], Loss: 1.5479\n",
      "Epoch [95 / 1000], Loss: 1.5477\n",
      "Epoch [96 / 1000], Loss: 1.5452\n",
      "Epoch [97 / 1000], Loss: 1.5439\n",
      "Epoch [98 / 1000], Loss: 1.5424\n",
      "Epoch [99 / 1000], Loss: 1.5413\n",
      "Epoch [100 / 1000], Loss: 1.5389\n",
      "Epoch [101 / 1000], Loss: 1.5369\n",
      "Mevostangnggwapstaphamptanodymabekesadochachidrmap\n",
      "Ingcadrphachustagnyphaxajisacosphemangtengngngngrm\n",
      "Jytosangnggwapstaphamptanodymabekesadochachidrmapp\n",
      "Macadrphachustagnyphaxajisacosphemangtengngngngrme\n",
      "Xsthangnggwapstaphamptanodymabekesadochachidrmapps\n",
      "Dechoscedosmphanyphaxajisacosphemangtengngngngrmen\n",
      "Tosangnggwapstaphamptanodymabekesadochachidrmappsa\n",
      "\n",
      "\n",
      "Epoch [102 / 1000], Loss: 1.5350\n",
      "Epoch [103 / 1000], Loss: 1.5343\n",
      "Epoch [104 / 1000], Loss: 1.5335\n",
      "Epoch [105 / 1000], Loss: 1.5319\n",
      "Epoch [106 / 1000], Loss: 1.5293\n",
      "Epoch [107 / 1000], Loss: 1.5286\n",
      "Epoch [108 / 1000], Loss: 1.5276\n",
      "Epoch [109 / 1000], Loss: 1.5261\n",
      "Epoch [110 / 1000], Loss: 1.5250\n",
      "Epoch [111 / 1000], Loss: 1.5240\n",
      "Mevostangnggwapstaphamptanodymabdeltadochachidrngn\n",
      "Ingcadrodedosmphanyphaxagnodesthanggngtengngngngps\n",
      "Jytosangnggwapstaphamptanodymabdeltadochachidrngng\n",
      "Macadrodedosmphanyphaxagnodesthanggngtengngngngpsp\n",
      "Xsthangnggwapstaphamptanodymabdeltadochachidrngngn\n",
      "Dechoscedosmphanyphaxagnodesthanggngtengngngngpspj\n",
      "Tosangnggwapstaphamptanodymabdeltadochachidrngngne\n",
      "\n",
      "\n",
      "Epoch [112 / 1000], Loss: 1.5221\n",
      "Epoch [113 / 1000], Loss: 1.5201\n",
      "Epoch [114 / 1000], Loss: 1.5192\n",
      "Epoch [115 / 1000], Loss: 1.5174\n",
      "Epoch [116 / 1000], Loss: 1.5162\n",
      "Epoch [117 / 1000], Loss: 1.5147\n",
      "Epoch [118 / 1000], Loss: 1.5124\n",
      "Epoch [119 / 1000], Loss: 1.5126\n",
      "Epoch [120 / 1000], Loss: 1.5111\n",
      "Epoch [121 / 1000], Loss: 1.5091\n",
      "Mevossangnggwapstaphamptanodymabejausagbfedisathap\n",
      "Helabastacepteschesphax\n",
      "Itossangnggwapstaphamptanodymabejausagbfedisathapp\n",
      "Macadrphachustafluscex\n",
      "Xsthangnggwapstaphamptanodymabejausagbfedisathapps\n",
      "Cechoscedoskschesphax\n",
      "Tosangnggwapstaphamptanodymabejausagbfedisathappsa\n",
      "\n",
      "\n",
      "Epoch [122 / 1000], Loss: 1.5087\n",
      "Epoch [123 / 1000], Loss: 1.5070\n",
      "Epoch [124 / 1000], Loss: 1.5066\n",
      "Epoch [125 / 1000], Loss: 1.5032\n",
      "Epoch [126 / 1000], Loss: 1.5040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [127 / 1000], Loss: 1.5033\n",
      "Epoch [128 / 1000], Loss: 1.5016\n",
      "Epoch [129 / 1000], Loss: 1.5009\n",
      "Epoch [130 / 1000], Loss: 1.4988\n",
      "Epoch [131 / 1000], Loss: 1.4991\n",
      "Mevossangnggvesptaphamptanodymabbagtadochachkathap\n",
      "Ingcadrodedoskschcrghax\n",
      "Jyvosangnggvesptaphamptanodymabbagtadochachkathapp\n",
      "Macadrodedoskschcrghax\n",
      "Xsthangnggvesptaphamptanodymabbagtadochachkathapps\n",
      "Cechoscedoskschcrghax\n",
      "Tosangnggvesptaphamptanodymabbagtadochachkathappsa\n",
      "\n",
      "\n",
      "Epoch [132 / 1000], Loss: 1.4962\n",
      "Epoch [133 / 1000], Loss: 1.4958\n",
      "Epoch [134 / 1000], Loss: 1.4955\n",
      "Epoch [135 / 1000], Loss: 1.4962\n",
      "Epoch [136 / 1000], Loss: 1.4940\n",
      "Epoch [137 / 1000], Loss: 1.4927\n",
      "Epoch [138 / 1000], Loss: 1.4909\n",
      "Epoch [139 / 1000], Loss: 1.4902\n",
      "Epoch [140 / 1000], Loss: 1.4890\n",
      "Epoch [141 / 1000], Loss: 1.4876\n",
      "Mdywthangnggvesptaphamptanodymabadosadochabangrmap\n",
      "Helabastacepteschcomactagnodesthanggngsphangngngon\n",
      "Itosqchanggvesptaphamptanodymabadosadochabangrmapp\n",
      "Macadrodedoskschcomactagnodesthanggngsphangngngong\n",
      "Ywthangnggvesptaphamptanodymabadosadochabangrmappr\n",
      "Cechoscedoskschcomactagnodesthanggngsphangngngongn\n",
      "Tosangnggvesptaphamptanodymabadosadochabangrmappra\n",
      "\n",
      "\n",
      "Epoch [142 / 1000], Loss: 1.4868\n",
      "Epoch [143 / 1000], Loss: 1.4872\n",
      "Epoch [144 / 1000], Loss: 1.4851\n",
      "Epoch [145 / 1000], Loss: 1.4857\n",
      "Epoch [146 / 1000], Loss: 1.4828\n",
      "Epoch [147 / 1000], Loss: 1.4814\n",
      "Epoch [148 / 1000], Loss: 1.4805\n",
      "Epoch [149 / 1000], Loss: 1.4814\n",
      "Epoch [150 / 1000], Loss: 1.4806\n",
      "Epoch [151 / 1000], Loss: 1.4789\n",
      "Mbusthangnggvesptaphamptanodyngbadosadododelnfusap\n",
      "Helabastacdoshusanyphax\n",
      "Itosphemiphylonochangos\n",
      "Macadrodedoshusanyphax\n",
      "Yvosangnggvesptaphamptanodyngbadosadododelnfusappr\n",
      "Cechoscedoshusanyphax\n",
      "Tosangnggvesptaphamptanodyngbadosadododelnfusappra\n",
      "\n",
      "\n",
      "Epoch [152 / 1000], Loss: 1.4775\n",
      "Epoch [153 / 1000], Loss: 1.4787\n",
      "Epoch [154 / 1000], Loss: 1.4762\n",
      "Epoch [155 / 1000], Loss: 1.4762\n",
      "Epoch [156 / 1000], Loss: 1.4737\n",
      "Epoch [157 / 1000], Loss: 1.4734\n",
      "Epoch [158 / 1000], Loss: 1.4718\n",
      "Epoch [159 / 1000], Loss: 1.4707\n",
      "Epoch [160 / 1000], Loss: 1.4698\n",
      "Epoch [161 / 1000], Loss: 1.4712\n",
      "Mbusthangnggvesptaphamps\n",
      "Hemabastacepteschavemax\n",
      "Jyvosangnggvesptaphamps\n",
      "Macadrodedoshusanyphax\n",
      "Xpthangnggvesptaphamps\n",
      "Cechoscedoshusanyphax\n",
      "Tosangnggvesptaphamps\n",
      "\n",
      "\n",
      "Epoch [162 / 1000], Loss: 1.4679\n",
      "Epoch [163 / 1000], Loss: 1.4688\n",
      "Epoch [164 / 1000], Loss: 1.4649\n",
      "Epoch [165 / 1000], Loss: 1.4657\n",
      "Epoch [166 / 1000], Loss: 1.4649\n",
      "Epoch [167 / 1000], Loss: 1.4643\n",
      "Epoch [168 / 1000], Loss: 1.4628\n",
      "Epoch [169 / 1000], Loss: 1.4637\n",
      "Epoch [170 / 1000], Loss: 1.4636\n",
      "Epoch [171 / 1000], Loss: 1.4592\n",
      "Mdyvosangnggvesptaphamps\n",
      "Ingdadrodedosischcrgictalosacosphengngrdodrmellbqs\n",
      "Jyvosangnggvesptaphamps\n",
      "Macadrodedosischcrgictalosacosphengngrdodrmellbqsp\n",
      "Xpthangnggvesptaphamps\n",
      "Dechoscedosischcrgictalosacosphengngrdodrmellbqsph\n",
      "Tosangnggvesptaphamps\n",
      "\n",
      "\n",
      "Epoch [172 / 1000], Loss: 1.4617\n",
      "Epoch [173 / 1000], Loss: 1.4600\n",
      "Epoch [174 / 1000], Loss: 1.4567\n",
      "Epoch [175 / 1000], Loss: 1.4577\n",
      "Epoch [176 / 1000], Loss: 1.4576\n",
      "Epoch [177 / 1000], Loss: 1.4570\n",
      "Epoch [178 / 1000], Loss: 1.4569\n",
      "Epoch [179 / 1000], Loss: 1.4581\n",
      "Epoch [180 / 1000], Loss: 1.4544\n",
      "Epoch [181 / 1000], Loss: 1.4547\n",
      "Mdytosangnggvesptaphanos\n",
      "Hhacadrodedoskragnyphax\n",
      "Itosphemnggvesptaphanos\n",
      "Macadrodedoskragnyphax\n",
      "Xpthangnggvesptaphanos\n",
      "Cechoscedoskragnyphax\n",
      "Tosangnggvesptaphanos\n",
      "\n",
      "\n",
      "Epoch [182 / 1000], Loss: 1.4534\n",
      "Epoch [183 / 1000], Loss: 1.4535\n",
      "Epoch [184 / 1000], Loss: 1.4529\n",
      "Epoch [185 / 1000], Loss: 1.4530\n",
      "Epoch [186 / 1000], Loss: 1.4503\n",
      "Epoch [187 / 1000], Loss: 1.4512\n",
      "Epoch [188 / 1000], Loss: 1.4492\n",
      "Epoch [189 / 1000], Loss: 1.4479\n",
      "Epoch [190 / 1000], Loss: 1.4495\n",
      "Epoch [191 / 1000], Loss: 1.4498\n",
      "Mbusthangnggusprochangos\n",
      "Ingdbastegbeshusanyphax\n",
      "Jxmpsangnggusprochangos\n",
      "Macadrodedoshusanyphax\n",
      "Xmpsangnggusprochangos\n",
      "Cechoscedoshusanyphax\n",
      "Tosangnggusprochangos\n",
      "\n",
      "\n",
      "Epoch [192 / 1000], Loss: 1.4475\n",
      "Epoch [193 / 1000], Loss: 1.4468\n",
      "Epoch [194 / 1000], Loss: 1.4466\n",
      "Epoch [195 / 1000], Loss: 1.4440\n",
      "Epoch [196 / 1000], Loss: 1.4451\n",
      "Epoch [197 / 1000], Loss: 1.4433\n",
      "Epoch [198 / 1000], Loss: 1.4419\n",
      "Epoch [199 / 1000], Loss: 1.4448\n",
      "Epoch [200 / 1000], Loss: 1.4421\n",
      "Epoch [201 / 1000], Loss: 1.4418\n",
      "Mbusthangnggvesptaphannianklyphangitaendodemidrmap\n",
      "Hemabastegbesesagnyphax\n",
      "Itosphemiphymonochangos\n",
      "Macadrodedosesagnyphax\n",
      "Xlthangnggvesptaphannianklyphangitaendodemidrmapon\n",
      "Cechosacchustengthadyglosacosphengngrangngngngongn\n",
      "Tosangnggvesptaphannianklyphangitaendodemidrmapong\n",
      "\n",
      "\n",
      "Epoch [202 / 1000], Loss: 1.4398\n",
      "Epoch [203 / 1000], Loss: 1.4404\n",
      "Epoch [204 / 1000], Loss: 1.4372\n",
      "Epoch [205 / 1000], Loss: 1.4402\n",
      "Epoch [206 / 1000], Loss: 1.4368\n",
      "Epoch [207 / 1000], Loss: 1.4373\n",
      "Epoch [208 / 1000], Loss: 1.4351\n",
      "Epoch [209 / 1000], Loss: 1.4359\n",
      "Epoch [210 / 1000], Loss: 1.4362\n",
      "Epoch [211 / 1000], Loss: 1.4342\n",
      "Mbusthangnggusprodrangos\n",
      "Hemabastegchustengsphax\n",
      "Itosphemingusprodrangos\n",
      "Macadrodedoshusanyphax\n",
      "Xlthangnggusprodrangos\n",
      "Cechosacchustengsphax\n",
      "Tosangnggusprodrangos\n",
      "\n",
      "\n",
      "Epoch [212 / 1000], Loss: 1.4370\n",
      "Epoch [213 / 1000], Loss: 1.4335\n",
      "Epoch [214 / 1000], Loss: 1.4352\n",
      "Epoch [215 / 1000], Loss: 1.4326\n",
      "Epoch [216 / 1000], Loss: 1.4322\n",
      "Epoch [217 / 1000], Loss: 1.4323\n",
      "Epoch [218 / 1000], Loss: 1.4314\n",
      "Epoch [219 / 1000], Loss: 1.4293\n",
      "Epoch [220 / 1000], Loss: 1.4303\n",
      "Epoch [221 / 1000], Loss: 1.4293\n",
      "Mbustidodrggtesptaphchus\n",
      "Ingddostejanodragnyphax\n",
      "Jxkrngngnggtesptaphchus\n",
      "Macadrodedosisbanyphax\n",
      "Xkrngngnggtesptaphchus\n",
      "Cechstejanodragnyphax\n",
      "Tosangnggtesptaphchus\n",
      "\n",
      "\n",
      "Epoch [222 / 1000], Loss: 1.4304\n",
      "Epoch [223 / 1000], Loss: 1.4311\n",
      "Epoch [224 / 1000], Loss: 1.4299\n",
      "Epoch [225 / 1000], Loss: 1.4289\n",
      "Epoch [226 / 1000], Loss: 1.4263\n",
      "Epoch [227 / 1000], Loss: 1.4288\n",
      "Epoch [228 / 1000], Loss: 1.4277\n",
      "Epoch [229 / 1000], Loss: 1.4272\n",
      "Epoch [230 / 1000], Loss: 1.4284\n",
      "Epoch [231 / 1000], Loss: 1.4236\n",
      "Mbusthangngguspps\n",
      "Ingddostelanodragnyphax\n",
      "Itosphijinguspps\n",
      "Macadrodedoskragnyphax\n",
      "Ysthangngguspps\n",
      "Dechtodedoskragnyphax\n",
      "Tosangngguspps\n",
      "\n",
      "\n",
      "Epoch [232 / 1000], Loss: 1.4252\n",
      "Epoch [233 / 1000], Loss: 1.4254\n",
      "Epoch [234 / 1000], Loss: 1.4251\n",
      "Epoch [235 / 1000], Loss: 1.4236\n",
      "Epoch [236 / 1000], Loss: 1.4232\n",
      "Epoch [237 / 1000], Loss: 1.4276\n",
      "Epoch [238 / 1000], Loss: 1.4212\n",
      "Epoch [239 / 1000], Loss: 1.4204\n",
      "Epoch [240 / 1000], Loss: 1.4228\n",
      "Epoch [241 / 1000], Loss: 1.4179\n",
      "Mbustidodrigtespsaphepspanimtendodosafedodemidrmap\n",
      "Ingddostelanodrajesphax\n",
      "Jwusphilmigtespsaphepspanimtendodosafedodemidrmapo\n",
      "Macadrodgchustengrendyglosacosphimangrchangngngong\n",
      "Xjusangnggtespsaphepspanimtendodosafedodemidrmapon\n",
      "Cechtodgchustengrendyglosacosphimangrchangngngongn\n",
      "Tosangnggtespsaphepspanimtendodosafedodemidrmapong\n",
      "\n",
      "\n",
      "Epoch [242 / 1000], Loss: 1.4213\n",
      "Epoch [243 / 1000], Loss: 1.4182\n",
      "Epoch [244 / 1000], Loss: 1.4193\n",
      "Epoch [245 / 1000], Loss: 1.4187\n",
      "Epoch [246 / 1000], Loss: 1.4186\n",
      "Epoch [247 / 1000], Loss: 1.4190\n",
      "Epoch [248 / 1000], Loss: 1.4184\n",
      "Epoch [249 / 1000], Loss: 1.4151\n",
      "Epoch [250 / 1000], Loss: 1.4148\n",
      "Epoch [251 / 1000], Loss: 1.4149\n",
      "Mbustododrikuspps\n",
      "Ingddostelanodrajhyphax\n",
      "Jwusphilmikuspps\n",
      "Macadrodgbeshusanyphax\n",
      "Xisphilmikuspps\n",
      "Cechtodgbeshusanyphax\n",
      "Tosangnggtesps\n",
      "\n",
      "\n",
      "Epoch [252 / 1000], Loss: 1.4145\n",
      "Epoch [253 / 1000], Loss: 1.4143\n",
      "Epoch [254 / 1000], Loss: 1.4145\n",
      "Epoch [255 / 1000], Loss: 1.4127\n",
      "Epoch [256 / 1000], Loss: 1.4134\n",
      "Epoch [257 / 1000], Loss: 1.4135\n",
      "Epoch [258 / 1000], Loss: 1.4100\n",
      "Epoch [259 / 1000], Loss: 1.4148\n",
      "Epoch [260 / 1000], Loss: 1.4147\n",
      "Epoch [261 / 1000], Loss: 1.4140\n",
      "Mbustigngnggsbonjaphepspaninzhabadosaffangbangprap\n",
      "Jilabastelanlesalesphax\n",
      "Kustigngnggsbonjaphepspaninzhabadosaffangbangprapn\n",
      "Macadrodedrphusanx\n",
      "Xisphilmilx\n",
      "Dechstelanlesalesphax\n",
      "Tosangnggsbonjaphepspaninzhabadosaffangbangprapngg\n",
      "\n",
      "\n",
      "Epoch [262 / 1000], Loss: 1.4143\n",
      "Epoch [263 / 1000], Loss: 1.4159\n",
      "Epoch [264 / 1000], Loss: 1.4104\n",
      "Epoch [265 / 1000], Loss: 1.4091\n",
      "Epoch [266 / 1000], Loss: 1.4116\n",
      "Epoch [267 / 1000], Loss: 1.4080\n",
      "Epoch [268 / 1000], Loss: 1.4083\n",
      "Epoch [269 / 1000], Loss: 1.4100\n",
      "Epoch [270 / 1000], Loss: 1.4107\n",
      "Epoch [271 / 1000], Loss: 1.4087\n",
      "Mbustododrhavesps\n",
      "Jilabastelanodrajhyphax\n",
      "Kustododrhavesps\n",
      "Macadrodedoshusanyphax\n",
      "Ystododrhavesps\n",
      "Dechtodedoshusanyphax\n",
      "Tosangnggtesps\n",
      "\n",
      "\n",
      "Epoch [272 / 1000], Loss: 1.4098\n",
      "Epoch [273 / 1000], Loss: 1.4052\n",
      "Epoch [274 / 1000], Loss: 1.4087\n",
      "Epoch [275 / 1000], Loss: 1.4082\n",
      "Epoch [276 / 1000], Loss: 1.4108\n",
      "Epoch [277 / 1000], Loss: 1.4085\n",
      "Epoch [278 / 1000], Loss: 1.4074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [279 / 1000], Loss: 1.4061\n",
      "Epoch [280 / 1000], Loss: 1.4038\n",
      "Epoch [281 / 1000], Loss: 1.4043\n",
      "Mbustododrhaveris\n",
      "Hilabasphabesclbanyphax\n",
      "Itosphilomaveris\n",
      "Macadrodfanodrakothactalosacosphimangrbalmingngong\n",
      "Xisphilomaveris\n",
      "Dechsphabesclbanyphax\n",
      "Tosangnggsasps\n",
      "\n",
      "\n",
      "Epoch [282 / 1000], Loss: 1.4036\n",
      "Epoch [283 / 1000], Loss: 1.4030\n",
      "Epoch [284 / 1000], Loss: 1.4052\n",
      "Epoch [285 / 1000], Loss: 1.4008\n",
      "Epoch [286 / 1000], Loss: 1.4050\n",
      "Epoch [287 / 1000], Loss: 1.4017\n",
      "Epoch [288 / 1000], Loss: 1.4010\n",
      "Epoch [289 / 1000], Loss: 1.4059\n",
      "Epoch [290 / 1000], Loss: 1.4028\n",
      "Epoch [291 / 1000], Loss: 1.4066\n",
      "Mbustkephongteris\n",
      "Hilabatodedosclbanyphax\n",
      "Itrosangnggteris\n",
      "Macadrodedosclbanyphax\n",
      "Ystkephongteris\n",
      "Cechrodedosclbanyphax\n",
      "Tosangnggteris\n",
      "\n",
      "\n",
      "Epoch [292 / 1000], Loss: 1.4013\n",
      "Epoch [293 / 1000], Loss: 1.3993\n",
      "Epoch [294 / 1000], Loss: 1.4463\n",
      "Epoch [295 / 1000], Loss: 1.4356\n",
      "Epoch [296 / 1000], Loss: 1.4076\n",
      "Epoch [297 / 1000], Loss: 1.4020\n",
      "Epoch [298 / 1000], Loss: 1.4052\n",
      "Epoch [299 / 1000], Loss: 1.4000\n",
      "Epoch [300 / 1000], Loss: 1.4006\n",
      "Epoch [301 / 1000], Loss: 1.4010\n",
      "Mbustigngnggterhumimannianjax\n",
      "Henddosphabavesaleskidygjisacoskhengngrangngngngon\n",
      "Itosphengngterhumimannianjax\n",
      "Macadrodefmphusanyphax\n",
      "Ystigngnggterhumimannianjax\n",
      "Cechosacchusphanyphax\n",
      "Tosangnggterhumimannianjax\n",
      "\n",
      "\n",
      "Epoch [302 / 1000], Loss: 1.3975\n",
      "Epoch [303 / 1000], Loss: 1.3987\n",
      "Epoch [304 / 1000], Loss: 1.3979\n",
      "Epoch [305 / 1000], Loss: 1.3961\n",
      "Epoch [306 / 1000], Loss: 1.3953\n",
      "Epoch [307 / 1000], Loss: 1.4008\n",
      "Epoch [308 / 1000], Loss: 1.3965\n",
      "Epoch [309 / 1000], Loss: 1.4011\n",
      "Epoch [310 / 1000], Loss: 1.3941\n",
      "Epoch [311 / 1000], Loss: 1.3948\n",
      "Mbustjangnggrasps\n",
      "Ingdemosacepsclaleskkdygkosacoskhimaphusallmbangos\n",
      "Jvosphengngrasps\n",
      "Macadrodhanodraleskkdygkosacoskhimaphusallmbangosh\n",
      "Xausangnggrasps\n",
      "Cedosphabesclaleskkdygkosacoskhimaphusallmbangoshb\n",
      "Tosangnggrasps\n",
      "\n",
      "\n",
      "Epoch [312 / 1000], Loss: 1.3937\n",
      "Epoch [313 / 1000], Loss: 1.3951\n",
      "Epoch [314 / 1000], Loss: 1.3960\n",
      "Epoch [315 / 1000], Loss: 1.3962\n",
      "Epoch [316 / 1000], Loss: 1.3914\n",
      "Epoch [317 / 1000], Loss: 1.3913\n",
      "Epoch [318 / 1000], Loss: 1.3963\n",
      "Epoch [319 / 1000], Loss: 1.3951\n",
      "Epoch [320 / 1000], Loss: 1.3952\n",
      "Epoch [321 / 1000], Loss: 1.3940\n",
      "Mbustododrimphops\n",
      "Ingdemosacepsclbanyphax\n",
      "Jwx\n",
      "Macadrodgbesclbanyphax\n",
      "Ystododrimphops\n",
      "Dechtodgbesclbanyphax\n",
      "Tosangnggrasps\n",
      "\n",
      "\n",
      "Epoch [322 / 1000], Loss: 1.3956\n",
      "Epoch [323 / 1000], Loss: 1.3944\n",
      "Epoch [324 / 1000], Loss: 1.3940\n",
      "Epoch [325 / 1000], Loss: 1.3927\n",
      "Epoch [326 / 1000], Loss: 1.3921\n",
      "Epoch [327 / 1000], Loss: 1.3909\n",
      "Epoch [328 / 1000], Loss: 1.4032\n",
      "Epoch [329 / 1000], Loss: 1.3938\n",
      "Epoch [330 / 1000], Loss: 1.3942\n",
      "Epoch [331 / 1000], Loss: 1.3922\n",
      "Mbustododrenykonodrangos\n",
      "Himabatodegnodralithactalosacosimengngrangngngngop\n",
      "Itosphferenykonodrangos\n",
      "Macadrodegnodralithactalosacosimengngrangngngngoph\n",
      "Xausangnggrasps\n",
      "Cedosphachustengrbactalosacosimengngrangngngngophe\n",
      "Tosangnggrasps\n",
      "\n",
      "\n",
      "Epoch [332 / 1000], Loss: 1.4002\n",
      "Epoch [333 / 1000], Loss: 1.3932\n",
      "Epoch [334 / 1000], Loss: 1.3887\n",
      "Epoch [335 / 1000], Loss: 1.3943\n",
      "Epoch [336 / 1000], Loss: 1.3883\n",
      "Epoch [337 / 1000], Loss: 1.3906\n",
      "Epoch [338 / 1000], Loss: 1.3891\n",
      "Epoch [339 / 1000], Loss: 1.3900\n",
      "Epoch [340 / 1000], Loss: 1.3872\n",
      "Epoch [341 / 1000], Loss: 1.3893\n",
      "Mbustilimingrasps\n",
      "Himabatodegnodraleshactalos\n",
      "Itosphengngrasps\n",
      "Macadrodegnodraleshactalos\n",
      "Ystilimingrasps\n",
      "Cedosphabausphanyphax\n",
      "Tosangnggrasps\n",
      "\n",
      "\n",
      "Epoch [342 / 1000], Loss: 1.3873\n",
      "Epoch [343 / 1000], Loss: 1.3836\n",
      "Epoch [344 / 1000], Loss: 1.3899\n",
      "Epoch [345 / 1000], Loss: 1.3869\n",
      "Epoch [346 / 1000], Loss: 1.3884\n",
      "Epoch [347 / 1000], Loss: 1.3889\n",
      "Epoch [348 / 1000], Loss: 1.3876\n",
      "Epoch [349 / 1000], Loss: 1.3846\n",
      "Epoch [350 / 1000], Loss: 1.3880\n",
      "Epoch [351 / 1000], Loss: 1.3880\n",
      "Mbuspsangnggrasps\n",
      "Ingbafsphabausphanyphax\n",
      "Juspsangnggrasps\n",
      "Macafsphabausphanyphax\n",
      "Yspsangnggrasps\n",
      "Cedosphabausphanyphax\n",
      "Tosangnggrasps\n",
      "\n",
      "\n",
      "Epoch [352 / 1000], Loss: 1.3815\n",
      "Epoch [353 / 1000], Loss: 1.3828\n",
      "Epoch [354 / 1000], Loss: 1.3992\n",
      "Epoch [355 / 1000], Loss: 1.3902\n",
      "Epoch [356 / 1000], Loss: 1.3889\n",
      "Epoch [357 / 1000], Loss: 1.3887\n",
      "Epoch [358 / 1000], Loss: 1.3877\n",
      "Epoch [359 / 1000], Loss: 1.3840\n",
      "Epoch [360 / 1000], Loss: 1.3824\n",
      "Epoch [361 / 1000], Loss: 1.3827\n",
      "Mbusprangnggrasmodrangos\n",
      "Himabausacepsausanyphax\n",
      "Jvosphimingrasmodrangos\n",
      "Macadrphachusphanyphax\n",
      "Ysprangnggrasmodrangos\n",
      "Cedosphachusphanyphax\n",
      "Tosangnggrasmodrangos\n",
      "\n",
      "\n",
      "Epoch [362 / 1000], Loss: 1.3873\n",
      "Epoch [363 / 1000], Loss: 1.3815\n",
      "Epoch [364 / 1000], Loss: 1.3831\n",
      "Epoch [365 / 1000], Loss: 1.3838\n",
      "Epoch [366 / 1000], Loss: 1.3892\n",
      "Epoch [367 / 1000], Loss: 1.3828\n",
      "Epoch [368 / 1000], Loss: 1.3965\n",
      "Epoch [369 / 1000], Loss: 1.3884\n",
      "Epoch [370 / 1000], Loss: 1.3833\n",
      "Epoch [371 / 1000], Loss: 1.3836\n",
      "Mbussphimingrathurododrochusthabados\n",
      "Ingcafrqidepsausanyphax\n",
      "Jvisphimingrathurododrochusthabados\n",
      "Macafrqidepsausanyphax\n",
      "Yssphimingrathurododrochusthabados\n",
      "Dedosphachusphanyphax\n",
      "Tosangnggrathurododrochusthabados\n",
      "\n",
      "\n",
      "Epoch [372 / 1000], Loss: 1.3824\n",
      "Epoch [373 / 1000], Loss: 1.3868\n",
      "Epoch [374 / 1000], Loss: 1.3866\n",
      "Epoch [375 / 1000], Loss: 1.3786\n",
      "Epoch [376 / 1000], Loss: 1.3800\n",
      "Epoch [377 / 1000], Loss: 1.3910\n",
      "Epoch [378 / 1000], Loss: 1.3802\n",
      "Epoch [379 / 1000], Loss: 1.3839\n",
      "Epoch [380 / 1000], Loss: 1.3780\n",
      "Epoch [381 / 1000], Loss: 1.3755\n",
      "Mbustillapodykonodrangos\n",
      "Himabausacepsavengrhactalos\n",
      "Itrusangnggrathungmbaus\n",
      "Macadrracepsavengrhactalos\n",
      "Ystillapodykonodrangos\n",
      "Cedmosacepsavengrhactalos\n",
      "Tosangnggrathungmbaus\n",
      "\n",
      "\n",
      "Epoch [382 / 1000], Loss: 1.3774\n",
      "Epoch [383 / 1000], Loss: 1.3835\n",
      "Epoch [384 / 1000], Loss: 1.3944\n",
      "Epoch [385 / 1000], Loss: 1.3834\n",
      "Epoch [386 / 1000], Loss: 1.3820\n",
      "Epoch [387 / 1000], Loss: 1.3807\n",
      "Epoch [388 / 1000], Loss: 1.3827\n",
      "Epoch [389 / 1000], Loss: 1.3736\n",
      "Epoch [390 / 1000], Loss: 1.3720\n",
      "Epoch [391 / 1000], Loss: 1.3856\n",
      "Mbussphiminfwathuqusanjnchthymabados\n",
      "Ingbadrsacchusphanyphax\n",
      "Jvosphiminfwathuqusanjnchthymabados\n",
      "Macadrsacchusphanyphax\n",
      "X\n",
      "Cedosphabavesalithactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [392 / 1000], Loss: 1.3807\n",
      "Epoch [393 / 1000], Loss: 1.3771\n",
      "Epoch [394 / 1000], Loss: 1.3735\n",
      "Epoch [395 / 1000], Loss: 1.3863\n",
      "Epoch [396 / 1000], Loss: 1.3803\n",
      "Epoch [397 / 1000], Loss: 1.3783\n",
      "Epoch [398 / 1000], Loss: 1.3835\n",
      "Epoch [399 / 1000], Loss: 1.3871\n",
      "Epoch [400 / 1000], Loss: 1.3729\n",
      "Epoch [401 / 1000], Loss: 1.3773\n",
      "Mbuspsangnggrathurmbanjmanjevepephaveng\n",
      "Ingdenisacepsausanyphax\n",
      "Juspsangnggrathurmbanjmanjevepephaveng\n",
      "Macadrphachusphanyphax\n",
      "X\n",
      "Cedosphachusphanyphax\n",
      "Tosangnggrathurmbanjmanjevepephaveng\n",
      "\n",
      "\n",
      "Epoch [402 / 1000], Loss: 1.3777\n",
      "Epoch [403 / 1000], Loss: 1.3718\n",
      "Epoch [404 / 1000], Loss: 1.3760\n",
      "Epoch [405 / 1000], Loss: 1.3760\n",
      "Epoch [406 / 1000], Loss: 1.3729\n",
      "Epoch [407 / 1000], Loss: 1.3753\n",
      "Epoch [408 / 1000], Loss: 1.3720\n",
      "Epoch [409 / 1000], Loss: 1.3776\n",
      "Epoch [410 / 1000], Loss: 1.3723\n",
      "Epoch [411 / 1000], Loss: 1.3705\n",
      "Mbusprangnggrathunglepspepsawalandos\n",
      "Himabausacepsavengrendygjjodesphanggngrbalophaphus\n",
      "Hyrusangnggrathunglepspepsawalandos\n",
      "Macadrqidepsavengrendygjjodesphanggngrbalophaphusb\n",
      "Yrusangnggrathunglepspepsawalandos\n",
      "Cedosphabavesakmphactalos\n",
      "Tosangnggrathunglepspepsawalandos\n",
      "\n",
      "\n",
      "Epoch [412 / 1000], Loss: 1.3731\n",
      "Epoch [413 / 1000], Loss: 1.3715\n",
      "Epoch [414 / 1000], Loss: 1.3761\n",
      "Epoch [415 / 1000], Loss: 1.3813\n",
      "Epoch [416 / 1000], Loss: 1.3729\n",
      "Epoch [417 / 1000], Loss: 1.3713\n",
      "Epoch [418 / 1000], Loss: 1.3707\n",
      "Epoch [419 / 1000], Loss: 1.3710\n",
      "Epoch [420 / 1000], Loss: 1.3718\n",
      "Epoch [421 / 1000], Loss: 1.3724\n",
      "Mbusps\n",
      "Jimabausacersavengrendygjkodesphanggngrbalophaphsg\n",
      "Ktrtododrhaveris\n",
      "Macadrphachusphanyphax\n",
      "X\n",
      "Dedosphachusphanyphax\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [422 / 1000], Loss: 1.3738\n",
      "Epoch [423 / 1000], Loss: 1.3679\n",
      "Epoch [424 / 1000], Loss: 1.3684\n",
      "Epoch [425 / 1000], Loss: 1.3703\n",
      "Epoch [426 / 1000], Loss: 1.3719\n",
      "Epoch [427 / 1000], Loss: 1.3714\n",
      "Epoch [428 / 1000], Loss: 1.3689\n",
      "Epoch [429 / 1000], Loss: 1.3700\n",
      "Epoch [430 / 1000], Loss: 1.3755\n",
      "Epoch [431 / 1000], Loss: 1.3713\n",
      "Mbustillapodykops\n",
      "Jimabausacerustengrfemtalos\n",
      "Ktrus\n",
      "Macadrqiderustengrfemtalos\n",
      "X\n",
      "Dedosphabavesalithactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [432 / 1000], Loss: 1.3901\n",
      "Epoch [433 / 1000], Loss: 1.3769\n",
      "Epoch [434 / 1000], Loss: 1.3679\n",
      "Epoch [435 / 1000], Loss: 1.3713\n",
      "Epoch [436 / 1000], Loss: 1.3665\n",
      "Epoch [437 / 1000], Loss: 1.3727\n",
      "Epoch [438 / 1000], Loss: 1.3725\n",
      "Epoch [439 / 1000], Loss: 1.3683\n",
      "Epoch [440 / 1000], Loss: 1.3735\n",
      "Epoch [441 / 1000], Loss: 1.3637\n",
      "Mbusprangnggrathulododrochthyklandos\n",
      "Himabausacerusphanyphax\n",
      "Itrus\n",
      "Macadrus\n",
      "Yrus\n",
      "Cedosphachusphanyphax\n",
      "Tos\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [442 / 1000], Loss: 1.3719\n",
      "Epoch [443 / 1000], Loss: 1.3795\n",
      "Epoch [444 / 1000], Loss: 1.3722\n",
      "Epoch [445 / 1000], Loss: 1.3720\n",
      "Epoch [446 / 1000], Loss: 1.3738\n",
      "Epoch [447 / 1000], Loss: 1.3695\n",
      "Epoch [448 / 1000], Loss: 1.3695\n",
      "Epoch [449 / 1000], Loss: 1.3691\n",
      "Epoch [450 / 1000], Loss: 1.3699\n",
      "Epoch [451 / 1000], Loss: 1.3672\n",
      "Mbusprangnggrausperangos\n",
      "Ingdenisacerusphanyphax\n",
      "Jusprangnggrausperangos\n",
      "Macadrsacerusphanyphax\n",
      "X\n",
      "Cedosphachusphanyphax\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [452 / 1000], Loss: 1.3764\n",
      "Epoch [453 / 1000], Loss: 1.3646\n",
      "Epoch [454 / 1000], Loss: 1.3691\n",
      "Epoch [455 / 1000], Loss: 1.3663\n",
      "Epoch [456 / 1000], Loss: 1.3709\n",
      "Epoch [457 / 1000], Loss: 1.3712\n",
      "Epoch [458 / 1000], Loss: 1.3709\n",
      "Epoch [459 / 1000], Loss: 1.3664\n",
      "Epoch [460 / 1000], Loss: 1.3620\n",
      "Epoch [461 / 1000], Loss: 1.3687\n",
      "Mbusps\n",
      "Himabausacerusphanyphax\n",
      "Itrus\n",
      "Macadrus\n",
      "Yrus\n",
      "Cedisphabavesalithactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [462 / 1000], Loss: 1.3658\n",
      "Epoch [463 / 1000], Loss: 1.3635\n",
      "Epoch [464 / 1000], Loss: 1.3650\n",
      "Epoch [465 / 1000], Loss: 1.3655\n",
      "Epoch [466 / 1000], Loss: 1.3688\n",
      "Epoch [467 / 1000], Loss: 1.3681\n",
      "Epoch [468 / 1000], Loss: 1.3676\n",
      "Epoch [469 / 1000], Loss: 1.3792\n",
      "Epoch [470 / 1000], Loss: 1.3636\n",
      "Epoch [471 / 1000], Loss: 1.3666\n",
      "Mbusprangnggrauspgngngos\n",
      "Jhacadrus\n",
      "Kusprangnggrauspgngngos\n",
      "Macadrus\n",
      "X\n",
      "Dedosphachusphanx\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [472 / 1000], Loss: 1.3722\n",
      "Epoch [473 / 1000], Loss: 1.3642\n",
      "Epoch [474 / 1000], Loss: 1.3704\n",
      "Epoch [475 / 1000], Loss: 1.3653\n",
      "Epoch [476 / 1000], Loss: 1.3705\n",
      "Epoch [477 / 1000], Loss: 1.3676\n",
      "Epoch [478 / 1000], Loss: 1.3660\n",
      "Epoch [479 / 1000], Loss: 1.3736\n",
      "Epoch [480 / 1000], Loss: 1.3674\n",
      "Epoch [481 / 1000], Loss: 1.3757\n",
      "Mestos\n",
      "Ingdengodhanjesallusacteprtacosbanggngqpharmenggop\n",
      "Jusps\n",
      "Macadrus\n",
      "X\n",
      "Dedosphabavesallusacteprtacosbanggngqpharmenggophe\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [482 / 1000], Loss: 1.3700\n",
      "Epoch [483 / 1000], Loss: 1.3626\n",
      "Epoch [484 / 1000], Loss: 1.3614\n",
      "Epoch [485 / 1000], Loss: 1.3692\n",
      "Epoch [486 / 1000], Loss: 1.3597\n",
      "Epoch [487 / 1000], Loss: 1.3608\n",
      "Epoch [488 / 1000], Loss: 1.3717\n",
      "Epoch [489 / 1000], Loss: 1.3638\n",
      "Epoch [490 / 1000], Loss: 1.3692\n",
      "Epoch [491 / 1000], Loss: 1.3753\n",
      "Mespsmangnggrausphododrochphyklandos\n",
      "Ingcadrphachusphanx\n",
      "Jusprangnggrausphododrochphyklandos\n",
      "Macadrphachusphanx\n",
      "Yrus\n",
      "Cedosphachusphanx\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [492 / 1000], Loss: 1.3640\n",
      "Epoch [493 / 1000], Loss: 1.3668\n",
      "Epoch [494 / 1000], Loss: 1.3668\n",
      "Epoch [495 / 1000], Loss: 1.3790\n",
      "Epoch [496 / 1000], Loss: 1.3725\n",
      "Epoch [497 / 1000], Loss: 1.3652\n",
      "Epoch [498 / 1000], Loss: 1.3600\n",
      "Epoch [499 / 1000], Loss: 1.3781\n",
      "Epoch [500 / 1000], Loss: 1.3606\n",
      "Epoch [501 / 1000], Loss: 1.3598\n",
      "Mbusps\n",
      "Ingdengodellusphanwphax\n",
      "Jusps\n",
      "Macadrphachusphanwphax\n",
      "X\n",
      "Cedosphachusphanwphax\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [502 / 1000], Loss: 1.3656\n",
      "Epoch [503 / 1000], Loss: 1.3662\n",
      "Epoch [504 / 1000], Loss: 1.3608\n",
      "Epoch [505 / 1000], Loss: 1.3579\n",
      "Epoch [506 / 1000], Loss: 1.3625\n",
      "Epoch [507 / 1000], Loss: 1.3549\n",
      "Epoch [508 / 1000], Loss: 1.3584\n",
      "Epoch [509 / 1000], Loss: 1.3637\n",
      "Epoch [510 / 1000], Loss: 1.3634\n",
      "Epoch [511 / 1000], Loss: 1.3591\n",
      "Messps\n",
      "Jephadrus\n",
      "Ktrus\n",
      "Macadrus\n",
      "X\n",
      "Dedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [512 / 1000], Loss: 1.3591\n",
      "Epoch [513 / 1000], Loss: 1.3590\n",
      "Epoch [514 / 1000], Loss: 1.3601\n",
      "Epoch [515 / 1000], Loss: 1.3651\n",
      "Epoch [516 / 1000], Loss: 1.3685\n",
      "Epoch [517 / 1000], Loss: 1.3597\n",
      "Epoch [518 / 1000], Loss: 1.3593\n",
      "Epoch [519 / 1000], Loss: 1.3609\n",
      "Epoch [520 / 1000], Loss: 1.3623\n",
      "Epoch [521 / 1000], Loss: 1.3593\n",
      "Litqus\n",
      "Hima\n",
      "Itqus\n",
      "Lacadrus\n",
      "Yrus\n",
      "Cedisphachusphanus\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [522 / 1000], Loss: 1.3708\n",
      "Epoch [523 / 1000], Loss: 1.3581\n",
      "Epoch [524 / 1000], Loss: 1.3570\n",
      "Epoch [525 / 1000], Loss: 1.3558\n",
      "Epoch [526 / 1000], Loss: 1.3836\n",
      "Epoch [527 / 1000], Loss: 1.3892\n",
      "Epoch [528 / 1000], Loss: 1.3627\n",
      "Epoch [529 / 1000], Loss: 1.3622\n",
      "Epoch [530 / 1000], Loss: 1.3703\n",
      "Epoch [531 / 1000], Loss: 1.3633\n",
      "Mestrododrimphosphojanis\n",
      "Jephachus\n",
      "Ktrus\n",
      "Macachus\n",
      "Yrus\n",
      "Dedosphachusphanthactalos\n",
      "Trododrimphosphojanis\n",
      "\n",
      "\n",
      "Epoch [532 / 1000], Loss: 1.3667\n",
      "Epoch [533 / 1000], Loss: 1.3630\n",
      "Epoch [534 / 1000], Loss: 1.3584\n",
      "Epoch [535 / 1000], Loss: 1.3600\n",
      "Epoch [536 / 1000], Loss: 1.3604\n",
      "Epoch [537 / 1000], Loss: 1.3597\n",
      "Epoch [538 / 1000], Loss: 1.3573\n",
      "Epoch [539 / 1000], Loss: 1.3540\n",
      "Epoch [540 / 1000], Loss: 1.3810\n",
      "Epoch [541 / 1000], Loss: 1.3633\n",
      "Messps\n",
      "Jendengodelosavephyphax\n",
      "Ktrus\n",
      "Macadrus\n",
      "Yrus\n",
      "Dedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [542 / 1000], Loss: 1.3757\n",
      "Epoch [543 / 1000], Loss: 1.3641\n",
      "Epoch [544 / 1000], Loss: 1.3645\n",
      "Epoch [545 / 1000], Loss: 1.3578\n",
      "Epoch [546 / 1000], Loss: 1.3538\n",
      "Epoch [547 / 1000], Loss: 1.3631\n",
      "Epoch [548 / 1000], Loss: 1.3659\n",
      "Epoch [549 / 1000], Loss: 1.3694\n",
      "Epoch [550 / 1000], Loss: 1.3576\n",
      "Epoch [551 / 1000], Loss: 1.3561\n",
      "Mesptilodrimphospholimps\n",
      "Kodedosphachusphanus\n",
      "Kusps\n",
      "Macadrus\n",
      "Yrus\n",
      "Dedosphachusphanus\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [552 / 1000], Loss: 1.3574\n",
      "Epoch [553 / 1000], Loss: 1.3745\n",
      "Epoch [554 / 1000], Loss: 1.3634\n",
      "Epoch [555 / 1000], Loss: 1.3561\n",
      "Epoch [556 / 1000], Loss: 1.3718\n",
      "Epoch [557 / 1000], Loss: 1.3606\n",
      "Epoch [558 / 1000], Loss: 1.3560\n",
      "Epoch [559 / 1000], Loss: 1.3587\n",
      "Epoch [560 / 1000], Loss: 1.3651\n",
      "Epoch [561 / 1000], Loss: 1.3548\n",
      "Mesqus\n",
      "Hima\n",
      "Itosilodrimphosperangos\n",
      "Macachus\n",
      "X\n",
      "Cedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [562 / 1000], Loss: 1.3551\n",
      "Epoch [563 / 1000], Loss: 1.3543\n",
      "Epoch [564 / 1000], Loss: 1.3499\n",
      "Epoch [565 / 1000], Loss: 1.3603\n",
      "Epoch [566 / 1000], Loss: 1.3569\n",
      "Epoch [567 / 1000], Loss: 1.3601\n",
      "Epoch [568 / 1000], Loss: 1.3781\n",
      "Epoch [569 / 1000], Loss: 1.3607\n",
      "Epoch [570 / 1000], Loss: 1.3555\n",
      "Epoch [571 / 1000], Loss: 1.3684\n",
      "Mesqus\n",
      "Ingdengodemesavephyphax\n",
      "Jusps\n",
      "Macachus\n",
      "Yrus\n",
      "Dedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [572 / 1000], Loss: 1.3560\n",
      "Epoch [573 / 1000], Loss: 1.3603\n",
      "Epoch [574 / 1000], Loss: 1.3613\n",
      "Epoch [575 / 1000], Loss: 1.3632\n",
      "Epoch [576 / 1000], Loss: 1.3572\n",
      "Epoch [577 / 1000], Loss: 1.3551\n",
      "Epoch [578 / 1000], Loss: 1.3761\n",
      "Epoch [579 / 1000], Loss: 1.3851\n",
      "Epoch [580 / 1000], Loss: 1.3526\n",
      "Epoch [581 / 1000], Loss: 1.3526\n",
      "Messps\n",
      "Ingbachus\n",
      "Jusps\n",
      "Macachus\n",
      "Yrus\n",
      "Cedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [582 / 1000], Loss: 1.3526\n",
      "Epoch [583 / 1000], Loss: 1.3686\n",
      "Epoch [584 / 1000], Loss: 1.3575\n",
      "Epoch [585 / 1000], Loss: 1.3579\n",
      "Epoch [586 / 1000], Loss: 1.3659\n",
      "Epoch [587 / 1000], Loss: 1.3624\n",
      "Epoch [588 / 1000], Loss: 1.3518\n",
      "Epoch [589 / 1000], Loss: 1.3650\n",
      "Epoch [590 / 1000], Loss: 1.3585\n",
      "Epoch [591 / 1000], Loss: 1.3544\n",
      "Mesptigngnggrausphododrperusyklandos\n",
      "Hima\n",
      "Itrus\n",
      "Macachus\n",
      "Yrus\n",
      "Cedisphachusphanthactagnjacosachachisalbathas\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [592 / 1000], Loss: 1.3535\n",
      "Epoch [593 / 1000], Loss: 1.3630\n",
      "Epoch [594 / 1000], Loss: 1.3530\n",
      "Epoch [595 / 1000], Loss: 1.3528\n",
      "Epoch [596 / 1000], Loss: 1.3578\n",
      "Epoch [597 / 1000], Loss: 1.3595\n",
      "Epoch [598 / 1000], Loss: 1.3605\n",
      "Epoch [599 / 1000], Loss: 1.3594\n",
      "Epoch [600 / 1000], Loss: 1.3484\n",
      "Epoch [601 / 1000], Loss: 1.3539\n",
      "Mesptikodrimphosphojanjkbavevepffeptachang\n",
      "Ing\n",
      "Jusps\n",
      "Macachus\n",
      "X\n",
      "Cedmos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [602 / 1000], Loss: 1.3515\n",
      "Epoch [603 / 1000], Loss: 1.3506\n",
      "Epoch [604 / 1000], Loss: 1.3504\n",
      "Epoch [605 / 1000], Loss: 1.3617\n",
      "Epoch [606 / 1000], Loss: 1.3697\n",
      "Epoch [607 / 1000], Loss: 1.3536\n",
      "Epoch [608 / 1000], Loss: 1.3511\n",
      "Epoch [609 / 1000], Loss: 1.3551\n",
      "Epoch [610 / 1000], Loss: 1.3503\n",
      "Epoch [611 / 1000], Loss: 1.3749\n",
      "Mesptilodrimphosphojanis\n",
      "Ing\n",
      "Jusprangnggrausphojanis\n",
      "Mba\n",
      "X\n",
      "Dedosphachusphansadiveprus\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [612 / 1000], Loss: 1.3605\n",
      "Epoch [613 / 1000], Loss: 1.3621\n",
      "Epoch [614 / 1000], Loss: 1.3641\n",
      "Epoch [615 / 1000], Loss: 1.3666\n",
      "Epoch [616 / 1000], Loss: 1.3524\n",
      "Epoch [617 / 1000], Loss: 1.3582\n",
      "Epoch [618 / 1000], Loss: 1.3542\n",
      "Epoch [619 / 1000], Loss: 1.3506\n",
      "Epoch [620 / 1000], Loss: 1.3504\n",
      "Epoch [621 / 1000], Loss: 1.3550\n",
      "Mesptijeprilwchlukolliscbavevepglajs\n",
      "Ing\n",
      "Jusps\n",
      "Mba\n",
      "Yrus\n",
      "Cedisphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [622 / 1000], Loss: 1.3489\n",
      "Epoch [623 / 1000], Loss: 1.3552\n",
      "Epoch [624 / 1000], Loss: 1.3474\n",
      "Epoch [625 / 1000], Loss: 1.3520\n",
      "Epoch [626 / 1000], Loss: 1.3440\n",
      "Epoch [627 / 1000], Loss: 1.3756\n",
      "Epoch [628 / 1000], Loss: 1.3615\n",
      "Epoch [629 / 1000], Loss: 1.3491\n",
      "Epoch [630 / 1000], Loss: 1.3489\n",
      "Epoch [631 / 1000], Loss: 1.3502\n",
      "Mesptododrimphospgngngos\n",
      "Ing\n",
      "Jusps\n",
      "Mba\n",
      "X\n",
      "Dedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [632 / 1000], Loss: 1.3546\n",
      "Epoch [633 / 1000], Loss: 1.3521\n",
      "Epoch [634 / 1000], Loss: 1.3518\n",
      "Epoch [635 / 1000], Loss: 1.3629\n",
      "Epoch [636 / 1000], Loss: 1.3662\n",
      "Epoch [637 / 1000], Loss: 1.3473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [638 / 1000], Loss: 1.3490\n",
      "Epoch [639 / 1000], Loss: 1.3502\n",
      "Epoch [640 / 1000], Loss: 1.3598\n",
      "Epoch [641 / 1000], Loss: 1.3434\n",
      "Mesptilodrhaveris\n",
      "Hima\n",
      "Itrus\n",
      "Macachus\n",
      "X\n",
      "Cedisphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [642 / 1000], Loss: 1.3430\n",
      "Epoch [643 / 1000], Loss: 1.3702\n",
      "Epoch [644 / 1000], Loss: 1.3683\n",
      "Epoch [645 / 1000], Loss: 1.3631\n",
      "Epoch [646 / 1000], Loss: 1.3553\n",
      "Epoch [647 / 1000], Loss: 1.3494\n",
      "Epoch [648 / 1000], Loss: 1.3548\n",
      "Epoch [649 / 1000], Loss: 1.3539\n",
      "Epoch [650 / 1000], Loss: 1.3529\n",
      "Epoch [651 / 1000], Loss: 1.3481\n",
      "Mespthangngfykosphphepskbavevephengrachang\n",
      "Ing\n",
      "Jusps\n",
      "Mba\n",
      "Yrus\n",
      "Dedosphachusphanthactakos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [652 / 1000], Loss: 1.3493\n",
      "Epoch [653 / 1000], Loss: 1.3551\n",
      "Epoch [654 / 1000], Loss: 1.3528\n",
      "Epoch [655 / 1000], Loss: 1.3472\n",
      "Epoch [656 / 1000], Loss: 1.3450\n",
      "Epoch [657 / 1000], Loss: 1.3570\n",
      "Epoch [658 / 1000], Loss: 1.3500\n",
      "Epoch [659 / 1000], Loss: 1.3524\n",
      "Epoch [660 / 1000], Loss: 1.3559\n",
      "Epoch [661 / 1000], Loss: 1.3499\n",
      "Messps\n",
      "Ing\n",
      "Jusps\n",
      "Mba\n",
      "X\n",
      "Cedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [662 / 1000], Loss: 1.3552\n",
      "Epoch [663 / 1000], Loss: 1.3596\n",
      "Epoch [664 / 1000], Loss: 1.3506\n",
      "Epoch [665 / 1000], Loss: 1.3521\n",
      "Epoch [666 / 1000], Loss: 1.3576\n",
      "Epoch [667 / 1000], Loss: 1.3517\n",
      "Epoch [668 / 1000], Loss: 1.3643\n",
      "Epoch [669 / 1000], Loss: 1.3558\n",
      "Epoch [670 / 1000], Loss: 1.3632\n",
      "Epoch [671 / 1000], Loss: 1.3443\n",
      "Mestos\n",
      "Ing\n",
      "Kusps\n",
      "Mba\n",
      "Yrus\n",
      "Dedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [672 / 1000], Loss: 1.3424\n",
      "Epoch [673 / 1000], Loss: 1.3424\n",
      "Epoch [674 / 1000], Loss: 1.3702\n",
      "Epoch [675 / 1000], Loss: 1.3459\n",
      "Epoch [676 / 1000], Loss: 1.3453\n",
      "Epoch [677 / 1000], Loss: 1.3562\n",
      "Epoch [678 / 1000], Loss: 1.3503\n",
      "Epoch [679 / 1000], Loss: 1.3478\n",
      "Epoch [680 / 1000], Loss: 1.3506\n",
      "Epoch [681 / 1000], Loss: 1.3574\n",
      "Messps\n",
      "Ing\n",
      "Jusps\n",
      "Mbabavodelosavephyphawalos\n",
      "Yrus\n",
      "Cedisphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [682 / 1000], Loss: 1.3581\n",
      "Epoch [683 / 1000], Loss: 1.3468\n",
      "Epoch [684 / 1000], Loss: 1.3453\n",
      "Epoch [685 / 1000], Loss: 1.3521\n",
      "Epoch [686 / 1000], Loss: 1.3529\n",
      "Epoch [687 / 1000], Loss: 1.3425\n",
      "Epoch [688 / 1000], Loss: 1.3464\n",
      "Epoch [689 / 1000], Loss: 1.3485\n",
      "Epoch [690 / 1000], Loss: 1.3433\n",
      "Epoch [691 / 1000], Loss: 1.3450\n",
      "Lotrus\n",
      "Hima\n",
      "Itrus\n",
      "Lemachus\n",
      "Yrus\n",
      "Cedisphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [692 / 1000], Loss: 1.3552\n",
      "Epoch [693 / 1000], Loss: 1.3437\n",
      "Epoch [694 / 1000], Loss: 1.3430\n",
      "Epoch [695 / 1000], Loss: 1.3516\n",
      "Epoch [696 / 1000], Loss: 1.3458\n",
      "Epoch [697 / 1000], Loss: 1.3636\n",
      "Epoch [698 / 1000], Loss: 1.3558\n",
      "Epoch [699 / 1000], Loss: 1.3480\n",
      "Epoch [700 / 1000], Loss: 1.3404\n",
      "Epoch [701 / 1000], Loss: 1.3384\n",
      "Mespthaphmilwchlukomepsianjavephachyggiaphachavepr\n",
      "Ing\n",
      "Juspraphmilwchlukomepsianjavephachyggiaphachavepre\n",
      "Mba\n",
      "Yrus\n",
      "Dedosphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [702 / 1000], Loss: 1.3530\n",
      "Epoch [703 / 1000], Loss: 1.3542\n",
      "Epoch [704 / 1000], Loss: 1.3415\n",
      "Epoch [705 / 1000], Loss: 1.3460\n",
      "Epoch [706 / 1000], Loss: 1.3424\n",
      "Epoch [707 / 1000], Loss: 1.3486\n",
      "Epoch [708 / 1000], Loss: 1.3466\n",
      "Epoch [709 / 1000], Loss: 1.3519\n",
      "Epoch [710 / 1000], Loss: 1.3692\n",
      "Epoch [711 / 1000], Loss: 1.3670\n",
      "Mesptilodrhaveris\n",
      "Ing\n",
      "Jusps\n",
      "Mba\n",
      "X\n",
      "Cedosphachusphansafdygjhus\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [712 / 1000], Loss: 1.3529\n",
      "Epoch [713 / 1000], Loss: 1.3538\n",
      "Epoch [714 / 1000], Loss: 1.3532\n",
      "Epoch [715 / 1000], Loss: 1.3593\n",
      "Epoch [716 / 1000], Loss: 1.3475\n",
      "Epoch [717 / 1000], Loss: 1.3434\n",
      "Epoch [718 / 1000], Loss: 1.3511\n",
      "Epoch [719 / 1000], Loss: 1.3568\n",
      "Epoch [720 / 1000], Loss: 1.3546\n",
      "Epoch [721 / 1000], Loss: 1.3519\n",
      "Mespthaphingrausphonglos\n",
      "Hima\n",
      "Itrus\n",
      "Mba\n",
      "Yrus\n",
      "Cedisphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [722 / 1000], Loss: 1.3481\n",
      "Epoch [723 / 1000], Loss: 1.3550\n",
      "Epoch [724 / 1000], Loss: 1.3543\n",
      "Epoch [725 / 1000], Loss: 1.3430\n",
      "Epoch [726 / 1000], Loss: 1.3492\n",
      "Epoch [727 / 1000], Loss: 1.3458\n",
      "Epoch [728 / 1000], Loss: 1.3447\n",
      "Epoch [729 / 1000], Loss: 1.3467\n",
      "Epoch [730 / 1000], Loss: 1.3553\n",
      "Epoch [731 / 1000], Loss: 1.3676\n",
      "Mesptilodrhaveris\n",
      "Ing\n",
      "Ktoscheppmbuschumomepscbavevepgjadrachaphachavenin\n",
      "Mba\n",
      "X\n",
      "Cedisphachusphansadgus\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [732 / 1000], Loss: 1.3593\n",
      "Epoch [733 / 1000], Loss: 1.3614\n",
      "Epoch [734 / 1000], Loss: 1.3443\n",
      "Epoch [735 / 1000], Loss: 1.3407\n",
      "Epoch [736 / 1000], Loss: 1.3520\n",
      "Epoch [737 / 1000], Loss: 1.3541\n",
      "Epoch [738 / 1000], Loss: 1.3513\n",
      "Epoch [739 / 1000], Loss: 1.3434\n",
      "Epoch [740 / 1000], Loss: 1.3701\n",
      "Epoch [741 / 1000], Loss: 1.3733\n",
      "Mesptilodrimphosphongnjkbaves\n",
      "Ing\n",
      "Kuspraphingrausphongnjkbaves\n",
      "Mba\n",
      "X\n",
      "Dedosphachusphanthactajis\n",
      "Trododrimphosphongnjkbaves\n",
      "\n",
      "\n",
      "Epoch [742 / 1000], Loss: 1.3610\n",
      "Epoch [743 / 1000], Loss: 1.3490\n",
      "Epoch [744 / 1000], Loss: 1.3497\n",
      "Epoch [745 / 1000], Loss: 1.3611\n",
      "Epoch [746 / 1000], Loss: 1.3459\n",
      "Epoch [747 / 1000], Loss: 1.3404\n",
      "Epoch [748 / 1000], Loss: 1.3349\n",
      "Epoch [749 / 1000], Loss: 1.3465\n",
      "Epoch [750 / 1000], Loss: 1.3485\n",
      "Epoch [751 / 1000], Loss: 1.3393\n",
      "Mestos\n",
      "Jephadrphachusphanws\n",
      "Kusps\n",
      "Mba\n",
      "Yrus\n",
      "Dedisphachusphanws\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [752 / 1000], Loss: 1.3370\n",
      "Epoch [753 / 1000], Loss: 1.3355\n",
      "Epoch [754 / 1000], Loss: 1.3533\n",
      "Epoch [755 / 1000], Loss: 1.3693\n",
      "Epoch [756 / 1000], Loss: 1.3574\n",
      "Epoch [757 / 1000], Loss: 1.3418\n",
      "Epoch [758 / 1000], Loss: 1.3527\n",
      "Epoch [759 / 1000], Loss: 1.3608\n",
      "Epoch [760 / 1000], Loss: 1.3411\n",
      "Epoch [761 / 1000], Loss: 1.3493\n",
      "Mestrodrathaveris\n",
      "Ing\n",
      "Kusps\n",
      "Mba\n",
      "X\n",
      "Cedisphachusphanthactalos\n",
      "Trodrathaveris\n",
      "\n",
      "\n",
      "Epoch [762 / 1000], Loss: 1.3451\n",
      "Epoch [763 / 1000], Loss: 1.3627\n",
      "Epoch [764 / 1000], Loss: 1.3524\n",
      "Epoch [765 / 1000], Loss: 1.3393\n",
      "Epoch [766 / 1000], Loss: 1.3457\n",
      "Epoch [767 / 1000], Loss: 1.3391\n",
      "Epoch [768 / 1000], Loss: 1.3450\n",
      "Epoch [769 / 1000], Loss: 1.3399\n",
      "Epoch [770 / 1000], Loss: 1.3799\n",
      "Epoch [771 / 1000], Loss: 1.3443\n",
      "Mesptilodrimphqus\n",
      "Hima\n",
      "Juspraqdrimphqus\n",
      "Mba\n",
      "Yrus\n",
      "Dedisphachusphanthactalos\n",
      "Trododrimphqus\n",
      "\n",
      "\n",
      "Epoch [772 / 1000], Loss: 1.3382\n",
      "Epoch [773 / 1000], Loss: 1.3454\n",
      "Epoch [774 / 1000], Loss: 1.3432\n",
      "Epoch [775 / 1000], Loss: 1.3360\n",
      "Epoch [776 / 1000], Loss: 1.3422\n",
      "Epoch [777 / 1000], Loss: 1.3472\n",
      "Epoch [778 / 1000], Loss: 1.3455\n",
      "Epoch [779 / 1000], Loss: 1.3574\n",
      "Epoch [780 / 1000], Loss: 1.3419\n",
      "Epoch [781 / 1000], Loss: 1.3435\n",
      "Mestos\n",
      "Ing\n",
      "Juspras\n",
      "Mba\n",
      "Yrpras\n",
      "Cedmos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [782 / 1000], Loss: 1.3359\n",
      "Epoch [783 / 1000], Loss: 1.3447\n",
      "Epoch [784 / 1000], Loss: 1.3566\n",
      "Epoch [785 / 1000], Loss: 1.3519\n",
      "Epoch [786 / 1000], Loss: 1.3421\n",
      "Epoch [787 / 1000], Loss: 1.3536\n",
      "Epoch [788 / 1000], Loss: 1.3387\n",
      "Epoch [789 / 1000], Loss: 1.3397\n",
      "Epoch [790 / 1000], Loss: 1.3455\n",
      "Epoch [791 / 1000], Loss: 1.3414\n",
      "Mesptilodrhaveris\n",
      "Hima\n",
      "Itspraqdrhaveris\n",
      "Mba\n",
      "Yrpraqdrhaveris\n",
      "Cedisphachusphanwus\n",
      "Trodraus\n",
      "\n",
      "\n",
      "Epoch [792 / 1000], Loss: 1.3376\n",
      "Epoch [793 / 1000], Loss: 1.3469\n",
      "Epoch [794 / 1000], Loss: 1.3694\n",
      "Epoch [795 / 1000], Loss: 1.3476\n",
      "Epoch [796 / 1000], Loss: 1.3522\n",
      "Epoch [797 / 1000], Loss: 1.3469\n",
      "Epoch [798 / 1000], Loss: 1.3437\n",
      "Epoch [799 / 1000], Loss: 1.3349\n",
      "Epoch [800 / 1000], Loss: 1.3463\n",
      "Epoch [801 / 1000], Loss: 1.3369\n",
      "Messpras\n",
      "Hfemachus\n",
      "Itrpras\n",
      "Mba\n",
      "Yrpras\n",
      "Cedisphachusphapthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [802 / 1000], Loss: 1.3624\n",
      "Epoch [803 / 1000], Loss: 1.3457\n",
      "Epoch [804 / 1000], Loss: 1.3427\n",
      "Epoch [805 / 1000], Loss: 1.3407\n",
      "Epoch [806 / 1000], Loss: 1.3395\n",
      "Epoch [807 / 1000], Loss: 1.3515\n",
      "Epoch [808 / 1000], Loss: 1.3531\n",
      "Epoch [809 / 1000], Loss: 1.3465\n",
      "Epoch [810 / 1000], Loss: 1.3430\n",
      "Epoch [811 / 1000], Loss: 1.3403\n",
      "Mesptilodrepthosphonggos\n",
      "Ing\n",
      "Juspras\n",
      "Mba\n",
      "X\n",
      "Cedosphachusphanthactalos\n",
      "Trodraus\n",
      "\n",
      "\n",
      "Epoch [812 / 1000], Loss: 1.3412\n",
      "Epoch [813 / 1000], Loss: 1.3506\n",
      "Epoch [814 / 1000], Loss: 1.3448\n",
      "Epoch [815 / 1000], Loss: 1.3412\n",
      "Epoch [816 / 1000], Loss: 1.3324\n",
      "Epoch [817 / 1000], Loss: 1.3506\n",
      "Epoch [818 / 1000], Loss: 1.3457\n",
      "Epoch [819 / 1000], Loss: 1.3375\n",
      "Epoch [820 / 1000], Loss: 1.3498\n",
      "Epoch [821 / 1000], Loss: 1.3355\n",
      "Messpras\n",
      "Hima\n",
      "Itrpras\n",
      "Mba\n",
      "Yrpras\n",
      "Dedosphachusphapthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [822 / 1000], Loss: 1.3500\n",
      "Epoch [823 / 1000], Loss: 1.3754\n",
      "Epoch [824 / 1000], Loss: 1.3483\n",
      "Epoch [825 / 1000], Loss: 1.3506\n",
      "Epoch [826 / 1000], Loss: 1.3466\n",
      "Epoch [827 / 1000], Loss: 1.3429\n",
      "Epoch [828 / 1000], Loss: 1.3460\n",
      "Epoch [829 / 1000], Loss: 1.3507\n",
      "Epoch [830 / 1000], Loss: 1.3558\n",
      "Epoch [831 / 1000], Loss: 1.3407\n",
      "Messpras\n",
      "Ing\n",
      "Juspras\n",
      "Mba\n",
      "X\n",
      "Cedosphachusphapthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [832 / 1000], Loss: 1.3561\n",
      "Epoch [833 / 1000], Loss: 1.3597\n",
      "Epoch [834 / 1000], Loss: 1.3457\n",
      "Epoch [835 / 1000], Loss: 1.3395\n",
      "Epoch [836 / 1000], Loss: 1.3326\n",
      "Epoch [837 / 1000], Loss: 1.3446\n",
      "Epoch [838 / 1000], Loss: 1.3393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [839 / 1000], Loss: 1.3326\n",
      "Epoch [840 / 1000], Loss: 1.3361\n",
      "Epoch [841 / 1000], Loss: 1.3441\n",
      "Mestrmaqdrepthosphonggos\n",
      "Ing\n",
      "Juspraqdrepthosphonggos\n",
      "Mba\n",
      "X\n",
      "Deferojachusphanthactalos\n",
      "Trmaqdrepthosphonggos\n",
      "\n",
      "\n",
      "Epoch [842 / 1000], Loss: 1.3442\n",
      "Epoch [843 / 1000], Loss: 1.3519\n",
      "Epoch [844 / 1000], Loss: 1.3456\n",
      "Epoch [845 / 1000], Loss: 1.3422\n",
      "Epoch [846 / 1000], Loss: 1.3423\n",
      "Epoch [847 / 1000], Loss: 1.3398\n",
      "Epoch [848 / 1000], Loss: 1.3629\n",
      "Epoch [849 / 1000], Loss: 1.3421\n",
      "Epoch [850 / 1000], Loss: 1.3415\n",
      "Epoch [851 / 1000], Loss: 1.3435\n",
      "Mestrmas\n",
      "Ing\n",
      "Kyros\n",
      "Mba\n",
      "X\n",
      "Deferolachusphapthactamis\n",
      "Trmas\n",
      "\n",
      "\n",
      "Epoch [852 / 1000], Loss: 1.3410\n",
      "Epoch [853 / 1000], Loss: 1.3445\n",
      "Epoch [854 / 1000], Loss: 1.3449\n",
      "Epoch [855 / 1000], Loss: 1.3348\n",
      "Epoch [856 / 1000], Loss: 1.3409\n",
      "Epoch [857 / 1000], Loss: 1.3383\n",
      "Epoch [858 / 1000], Loss: 1.3644\n",
      "Epoch [859 / 1000], Loss: 1.3522\n",
      "Epoch [860 / 1000], Loss: 1.3488\n",
      "Epoch [861 / 1000], Loss: 1.3431\n",
      "Messpraqdrepthosphonggos\n",
      "Hima\n",
      "Ityrmaqdrepthosphonggos\n",
      "Mba\n",
      "Yros\n",
      "Cedisphachusphanthactalos\n",
      "Trmaqdrepthosphonggos\n",
      "\n",
      "\n",
      "Epoch [862 / 1000], Loss: 1.3394\n",
      "Epoch [863 / 1000], Loss: 1.3666\n",
      "Epoch [864 / 1000], Loss: 1.3498\n",
      "Epoch [865 / 1000], Loss: 1.3471\n",
      "Epoch [866 / 1000], Loss: 1.3664\n",
      "Epoch [867 / 1000], Loss: 1.3447\n",
      "Epoch [868 / 1000], Loss: 1.3435\n",
      "Epoch [869 / 1000], Loss: 1.3395\n",
      "Epoch [870 / 1000], Loss: 1.3465\n",
      "Epoch [871 / 1000], Loss: 1.3336\n",
      "Lostrmas\n",
      "Hhacachus\n",
      "Itros\n",
      "Lemachus\n",
      "Yros\n",
      "Cedisphachusphapthactalos\n",
      "Trmas\n",
      "\n",
      "\n",
      "Epoch [872 / 1000], Loss: 1.3333\n",
      "Epoch [873 / 1000], Loss: 1.3320\n",
      "Epoch [874 / 1000], Loss: 1.3420\n",
      "Epoch [875 / 1000], Loss: 1.3375\n",
      "Epoch [876 / 1000], Loss: 1.3384\n",
      "Epoch [877 / 1000], Loss: 1.3428\n",
      "Epoch [878 / 1000], Loss: 1.3433\n",
      "Epoch [879 / 1000], Loss: 1.3396\n",
      "Epoch [880 / 1000], Loss: 1.3438\n",
      "Epoch [881 / 1000], Loss: 1.3530\n",
      "Mestos\n",
      "Ing\n",
      "Justilodrepththurimeros\n",
      "Mba\n",
      "X\n",
      "Cedisphachusphanthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [882 / 1000], Loss: 1.3647\n",
      "Epoch [883 / 1000], Loss: 1.3409\n",
      "Epoch [884 / 1000], Loss: 1.3435\n",
      "Epoch [885 / 1000], Loss: 1.3398\n",
      "Epoch [886 / 1000], Loss: 1.3940\n",
      "Epoch [887 / 1000], Loss: 1.3759\n",
      "Epoch [888 / 1000], Loss: 1.3613\n",
      "Epoch [889 / 1000], Loss: 1.3574\n",
      "Epoch [890 / 1000], Loss: 1.3354\n",
      "Epoch [891 / 1000], Loss: 1.3477\n",
      "Mesptilodrimphusphus\n",
      "Ing\n",
      "Juspras\n",
      "Mba\n",
      "X\n",
      "Dhachus\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [892 / 1000], Loss: 1.3378\n",
      "Epoch [893 / 1000], Loss: 1.3403\n",
      "Epoch [894 / 1000], Loss: 1.3469\n",
      "Epoch [895 / 1000], Loss: 1.3593\n",
      "Epoch [896 / 1000], Loss: 1.3492\n",
      "Epoch [897 / 1000], Loss: 1.3473\n",
      "Epoch [898 / 1000], Loss: 1.3515\n",
      "Epoch [899 / 1000], Loss: 1.3335\n",
      "Epoch [900 / 1000], Loss: 1.3393\n",
      "Epoch [901 / 1000], Loss: 1.3554\n",
      "Mesptododrepththuqus\n",
      "Ing\n",
      "Juspraqdrepththuqus\n",
      "Mba\n",
      "X\n",
      "Dhachus\n",
      "Trmaqdrepththuqus\n",
      "\n",
      "\n",
      "Epoch [902 / 1000], Loss: 1.3446\n",
      "Epoch [903 / 1000], Loss: 1.3382\n",
      "Epoch [904 / 1000], Loss: 1.3376\n",
      "Epoch [905 / 1000], Loss: 1.3346\n",
      "Epoch [906 / 1000], Loss: 1.3331\n",
      "Epoch [907 / 1000], Loss: 1.3469\n",
      "Epoch [908 / 1000], Loss: 1.3466\n",
      "Epoch [909 / 1000], Loss: 1.3491\n",
      "Epoch [910 / 1000], Loss: 1.3447\n",
      "Epoch [911 / 1000], Loss: 1.3600\n",
      "Mesptilojfles\n",
      "Ing\n",
      "Juspras\n",
      "Mba\n",
      "X\n",
      "Cedisphachusphapthactalos\n",
      "Trmas\n",
      "\n",
      "\n",
      "Epoch [912 / 1000], Loss: 1.3552\n",
      "Epoch [913 / 1000], Loss: 1.3606\n",
      "Epoch [914 / 1000], Loss: 1.3408\n",
      "Epoch [915 / 1000], Loss: 1.3458\n",
      "Epoch [916 / 1000], Loss: 1.3346\n",
      "Epoch [917 / 1000], Loss: 1.3320\n",
      "Epoch [918 / 1000], Loss: 1.3705\n",
      "Epoch [919 / 1000], Loss: 1.3959\n",
      "Epoch [920 / 1000], Loss: 1.3514\n",
      "Epoch [921 / 1000], Loss: 1.3439\n",
      "Messpras\n",
      "Hima\n",
      "Juspras\n",
      "Mba\n",
      "X\n",
      "Dhacis\n",
      "Trmas\n",
      "\n",
      "\n",
      "Epoch [922 / 1000], Loss: 1.3532\n",
      "Epoch [923 / 1000], Loss: 1.3433\n",
      "Epoch [924 / 1000], Loss: 1.3415\n",
      "Epoch [925 / 1000], Loss: 1.3654\n",
      "Epoch [926 / 1000], Loss: 1.3663\n",
      "Epoch [927 / 1000], Loss: 1.3448\n",
      "Epoch [928 / 1000], Loss: 1.3344\n",
      "Epoch [929 / 1000], Loss: 1.3256\n",
      "Epoch [930 / 1000], Loss: 1.3295\n",
      "Epoch [931 / 1000], Loss: 1.3286\n",
      "Mestos\n",
      "Hima\n",
      "Juspraqdrenykosphthanjiaquszs\n",
      "Mba\n",
      "X\n",
      "Cedisphachusphanthactajhus\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [932 / 1000], Loss: 1.3316\n",
      "Epoch [933 / 1000], Loss: 1.3322\n",
      "Epoch [934 / 1000], Loss: 1.3495\n",
      "Epoch [935 / 1000], Loss: 1.3449\n",
      "Epoch [936 / 1000], Loss: 1.3449\n",
      "Epoch [937 / 1000], Loss: 1.3408\n",
      "Epoch [938 / 1000], Loss: 1.3368\n",
      "Epoch [939 / 1000], Loss: 1.3297\n",
      "Epoch [940 / 1000], Loss: 1.3748\n",
      "Epoch [941 / 1000], Loss: 1.3998\n",
      "Mestos\n",
      "Ing\n",
      "Juspras\n",
      "Mba\n",
      "Yspras\n",
      "Ceffromachusphapthactalos\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [942 / 1000], Loss: 1.3623\n",
      "Epoch [943 / 1000], Loss: 1.3329\n",
      "Epoch [944 / 1000], Loss: 1.3691\n",
      "Epoch [945 / 1000], Loss: 1.3512\n",
      "Epoch [946 / 1000], Loss: 1.3398\n",
      "Epoch [947 / 1000], Loss: 1.3402\n",
      "Epoch [948 / 1000], Loss: 1.3404\n",
      "Epoch [949 / 1000], Loss: 1.3421\n",
      "Epoch [950 / 1000], Loss: 1.3527\n",
      "Epoch [951 / 1000], Loss: 1.3409\n",
      "Mestos\n",
      "Hffdenjllachusphanthactagnjacosachachisangnjachave\n",
      "Itros\n",
      "Mba\n",
      "X\n",
      "Cedisphachusphanthactagnjacosachachisangnjachavera\n",
      "Tos\n",
      "\n",
      "\n",
      "Epoch [952 / 1000], Loss: 1.3372\n",
      "Epoch [953 / 1000], Loss: 1.3359\n",
      "Epoch [954 / 1000], Loss: 1.3285\n",
      "Epoch [955 / 1000], Loss: 1.3353\n",
      "Epoch [956 / 1000], Loss: 1.3327\n",
      "Epoch [957 / 1000], Loss: 1.3398\n",
      "Epoch [958 / 1000], Loss: 1.3303\n",
      "Epoch [959 / 1000], Loss: 1.3291\n",
      "Epoch [960 / 1000], Loss: 1.3509\n",
      "Epoch [961 / 1000], Loss: 1.3370\n",
      "Losptilodrenykosphongfrmaqusus\n",
      "Hima\n",
      "Itros\n",
      "Lg\n",
      "Yros\n",
      "Cedisphachusphansadiveplodisphas\n",
      "Trjaqchimphosphongfrmaqusus\n",
      "\n",
      "\n",
      "Epoch [962 / 1000], Loss: 1.3457\n",
      "Epoch [963 / 1000], Loss: 1.3340\n",
      "Epoch [964 / 1000], Loss: 1.3373\n",
      "Epoch [965 / 1000], Loss: 1.3442\n",
      "Epoch [966 / 1000], Loss: 1.3379\n",
      "Epoch [967 / 1000], Loss: 1.3533\n",
      "Epoch [968 / 1000], Loss: 1.3379\n",
      "Epoch [969 / 1000], Loss: 1.3448\n",
      "Epoch [970 / 1000], Loss: 1.3367\n",
      "Epoch [971 / 1000], Loss: 1.3299\n",
      "Mesptododrbaveris\n",
      "Hima\n",
      "Juspraqdrbaveris\n",
      "Mba\n",
      "Yros\n",
      "Ceffrojachusphanthactagnjacosachachis\n",
      "Trilodrbaveris\n",
      "\n",
      "\n",
      "Epoch [972 / 1000], Loss: 1.3351\n",
      "Epoch [973 / 1000], Loss: 1.3296\n",
      "Epoch [974 / 1000], Loss: 1.3362\n",
      "Epoch [975 / 1000], Loss: 1.3400\n",
      "Epoch [976 / 1000], Loss: 1.3401\n",
      "Epoch [977 / 1000], Loss: 1.3372\n",
      "Epoch [978 / 1000], Loss: 1.3607\n",
      "Epoch [979 / 1000], Loss: 1.3544\n",
      "Epoch [980 / 1000], Loss: 1.3338\n",
      "Epoch [981 / 1000], Loss: 1.3251\n",
      "Mesptodrathaveris\n",
      "Jephachus\n",
      "Kyros\n",
      "Mba\n",
      "X\n",
      "Diachus\n",
      "Trmaqdrhaveris\n",
      "\n",
      "\n",
      "Epoch [982 / 1000], Loss: 1.3231\n",
      "Epoch [983 / 1000], Loss: 1.3330\n",
      "Epoch [984 / 1000], Loss: 1.3353\n",
      "Epoch [985 / 1000], Loss: 1.3421\n",
      "Epoch [986 / 1000], Loss: 1.3425\n",
      "Epoch [987 / 1000], Loss: 1.3307\n",
      "Epoch [988 / 1000], Loss: 1.3521\n",
      "Epoch [989 / 1000], Loss: 1.3542\n",
      "Epoch [990 / 1000], Loss: 1.3472\n",
      "Epoch [991 / 1000], Loss: 1.3298\n",
      "Mesptilodrepthqtodrangnjapravephachyg\n",
      "Hima\n",
      "Jvoschephimphqtodrangnjapravephachyg\n",
      "Mba\n",
      "Yros\n",
      "Ceffrolachusphanus\n",
      "Trilodrepthqtodrangnjapravephachyg\n",
      "\n",
      "\n",
      "Epoch [992 / 1000], Loss: 1.3278\n",
      "Epoch [993 / 1000], Loss: 1.3437\n",
      "Epoch [994 / 1000], Loss: 1.3335\n",
      "Epoch [995 / 1000], Loss: 1.3409\n",
      "Epoch [996 / 1000], Loss: 1.3312\n",
      "Epoch [997 / 1000], Loss: 1.3350\n",
      "Epoch [998 / 1000], Loss: 1.3286\n",
      "Epoch [999 / 1000], Loss: 1.3264\n",
      "Epoch [1000 / 1000], Loss: 1.3505\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "optimize(model, train_loader, num_epochs, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23de2f9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
