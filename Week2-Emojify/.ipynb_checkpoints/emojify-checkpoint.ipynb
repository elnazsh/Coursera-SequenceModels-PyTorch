{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10104655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from emo_utils import *\n",
    "from test_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3795c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¤ï¸\n",
      "âš¾\n",
      "ðŸ˜„\n",
      "ðŸ˜ž\n",
      "ðŸ´\n"
     ]
    }
   ],
   "source": [
    "emoji_dictionary = {0: \":red_heart:\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    1: \":baseball:\",\n",
    "                    2: \":smile:\",\n",
    "                    3: \":disappointed:\",\n",
    "                    4: \":fork_and_knife:\"}\n",
    "\n",
    "def my_label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[label], language=\"alias\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(my_label_to_emoji(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09464829",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298e4fc",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f3e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence):\n",
    "    \n",
    "    avg = np.zeros(VEC_SHAPE)\n",
    "    \n",
    "    count = 0\n",
    "    for word in sentence.lower().split():\n",
    "        if word in word_to_vec_map:\n",
    "            avg += word_to_vec_map[word]\n",
    "            count += 1\n",
    "\n",
    "    if count > 0: avg /= count\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e77248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        super(Data, self).__init__()\n",
    "        data = pd.read_csv(filename, header=None)\n",
    "        self.X, self.Y = data[0], data[1].astype(\"int\")\n",
    "        self.avg = torch.FloatTensor(np.stack(self.X.apply(sentence_to_avg)))\n",
    "        self.Y1h = F.one_hot(torch.tensor(self.Y.to_numpy()), NUM_CLASSES).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.avg[index], self.Y1h[index]\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.constant_(self.linear.bias, 0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70534bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_word = list(word_to_vec_map.keys())[0]\n",
    "VEC_SHAPE = word_to_vec_map[any_word].shape\n",
    "VEC_DIM = VEC_SHAPE[0]\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71d6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = Data(\"data/train_emoji.csv\")\n",
    "data_test = Data(\"data/test_emoji.csv\")\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(data_train, batch_size=1, shuffle=True)\n",
    "\n",
    "model = Model(VEC_DIM, NUM_CLASSES)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d695f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader,\n",
    "          criterion, optimizer, num_epochs):\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(x)\n",
    "            loss = criterion(yhat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch{epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        losses.append(epoch_loss)\n",
    "    \n",
    "        if epoch % 100 == 0 or epoch==num_epochs-1:\n",
    "            tp = 0\n",
    "            for xx, yy in test_loader:    \n",
    "                yyhat = model(xx)\n",
    "                if yyhat.argmax() == yy.argmax(): tp+=1\n",
    "            print(f\"Accuracy: {tp/len(test_loader):.4f}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1773fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1/400, Loss: 1.5049\n",
      "Accuracy: 0.3864\n",
      "Epoch2/400, Loss: 1.3859\n",
      "Epoch3/400, Loss: 1.3151\n",
      "Epoch4/400, Loss: 1.2229\n",
      "Epoch5/400, Loss: 1.1813\n",
      "Epoch6/400, Loss: 1.1155\n",
      "Epoch7/400, Loss: 1.0588\n",
      "Epoch8/400, Loss: 1.0268\n",
      "Epoch9/400, Loss: 1.0020\n",
      "Epoch10/400, Loss: 0.9658\n",
      "Epoch11/400, Loss: 0.9373\n",
      "Epoch12/400, Loss: 0.9119\n",
      "Epoch13/400, Loss: 0.8881\n",
      "Epoch14/400, Loss: 0.8614\n",
      "Epoch15/400, Loss: 0.8514\n",
      "Epoch16/400, Loss: 0.8314\n",
      "Epoch17/400, Loss: 0.7980\n",
      "Epoch18/400, Loss: 0.7972\n",
      "Epoch19/400, Loss: 0.7846\n",
      "Epoch20/400, Loss: 0.7636\n",
      "Epoch21/400, Loss: 0.7443\n",
      "Epoch22/400, Loss: 0.7415\n",
      "Epoch23/400, Loss: 0.7306\n",
      "Epoch24/400, Loss: 0.7177\n",
      "Epoch25/400, Loss: 0.7072\n",
      "Epoch26/400, Loss: 0.6925\n",
      "Epoch27/400, Loss: 0.6805\n",
      "Epoch28/400, Loss: 0.6817\n",
      "Epoch29/400, Loss: 0.6724\n",
      "Epoch30/400, Loss: 0.6597\n",
      "Epoch31/400, Loss: 0.6559\n",
      "Epoch32/400, Loss: 0.6438\n",
      "Epoch33/400, Loss: 0.6438\n",
      "Epoch34/400, Loss: 0.6318\n",
      "Epoch35/400, Loss: 0.6262\n",
      "Epoch36/400, Loss: 0.6097\n",
      "Epoch37/400, Loss: 0.6040\n",
      "Epoch38/400, Loss: 0.6043\n",
      "Epoch39/400, Loss: 0.5973\n",
      "Epoch40/400, Loss: 0.5916\n",
      "Epoch41/400, Loss: 0.5890\n",
      "Epoch42/400, Loss: 0.5803\n",
      "Epoch43/400, Loss: 0.5787\n",
      "Epoch44/400, Loss: 0.5665\n",
      "Epoch45/400, Loss: 0.5609\n",
      "Epoch46/400, Loss: 0.5599\n",
      "Epoch47/400, Loss: 0.5534\n",
      "Epoch48/400, Loss: 0.5476\n",
      "Epoch49/400, Loss: 0.5433\n",
      "Epoch50/400, Loss: 0.5446\n",
      "Epoch51/400, Loss: 0.5338\n",
      "Epoch52/400, Loss: 0.5346\n",
      "Epoch53/400, Loss: 0.5276\n",
      "Epoch54/400, Loss: 0.5240\n",
      "Epoch55/400, Loss: 0.5194\n",
      "Epoch56/400, Loss: 0.5179\n",
      "Epoch57/400, Loss: 0.5028\n",
      "Epoch58/400, Loss: 0.4980\n",
      "Epoch59/400, Loss: 0.5096\n",
      "Epoch60/400, Loss: 0.4986\n",
      "Epoch61/400, Loss: 0.4976\n",
      "Epoch62/400, Loss: 0.4906\n",
      "Epoch63/400, Loss: 0.4879\n",
      "Epoch64/400, Loss: 0.4909\n",
      "Epoch65/400, Loss: 0.4829\n",
      "Epoch66/400, Loss: 0.4822\n",
      "Epoch67/400, Loss: 0.4734\n",
      "Epoch68/400, Loss: 0.4801\n",
      "Epoch69/400, Loss: 0.4712\n",
      "Epoch70/400, Loss: 0.4604\n",
      "Epoch71/400, Loss: 0.4696\n",
      "Epoch72/400, Loss: 0.4574\n",
      "Epoch73/400, Loss: 0.4645\n",
      "Epoch74/400, Loss: 0.4575\n",
      "Epoch75/400, Loss: 0.4572\n",
      "Epoch76/400, Loss: 0.4495\n",
      "Epoch77/400, Loss: 0.4464\n",
      "Epoch78/400, Loss: 0.4442\n",
      "Epoch79/400, Loss: 0.4460\n",
      "Epoch80/400, Loss: 0.4414\n",
      "Epoch81/400, Loss: 0.4386\n",
      "Epoch82/400, Loss: 0.4381\n",
      "Epoch83/400, Loss: 0.4309\n",
      "Epoch84/400, Loss: 0.4344\n",
      "Epoch85/400, Loss: 0.4309\n",
      "Epoch86/400, Loss: 0.4284\n",
      "Epoch87/400, Loss: 0.4234\n",
      "Epoch88/400, Loss: 0.4171\n",
      "Epoch89/400, Loss: 0.4186\n",
      "Epoch90/400, Loss: 0.4203\n",
      "Epoch91/400, Loss: 0.4196\n",
      "Epoch92/400, Loss: 0.4136\n",
      "Epoch93/400, Loss: 0.4162\n",
      "Epoch94/400, Loss: 0.4129\n",
      "Epoch95/400, Loss: 0.4104\n",
      "Epoch96/400, Loss: 0.4025\n",
      "Epoch97/400, Loss: 0.4077\n",
      "Epoch98/400, Loss: 0.4036\n",
      "Epoch99/400, Loss: 0.4017\n",
      "Epoch100/400, Loss: 0.3992\n",
      "Epoch101/400, Loss: 0.3937\n",
      "Accuracy: 0.9091\n",
      "Epoch102/400, Loss: 0.3975\n",
      "Epoch103/400, Loss: 0.3936\n",
      "Epoch104/400, Loss: 0.3935\n",
      "Epoch105/400, Loss: 0.3929\n",
      "Epoch106/400, Loss: 0.3888\n",
      "Epoch107/400, Loss: 0.3797\n",
      "Epoch108/400, Loss: 0.3893\n",
      "Epoch109/400, Loss: 0.3845\n",
      "Epoch110/400, Loss: 0.3803\n",
      "Epoch111/400, Loss: 0.3837\n",
      "Epoch112/400, Loss: 0.3811\n",
      "Epoch113/400, Loss: 0.3770\n",
      "Epoch114/400, Loss: 0.3774\n",
      "Epoch115/400, Loss: 0.3737\n",
      "Epoch116/400, Loss: 0.3695\n",
      "Epoch117/400, Loss: 0.3717\n",
      "Epoch118/400, Loss: 0.3697\n",
      "Epoch119/400, Loss: 0.3709\n",
      "Epoch120/400, Loss: 0.3688\n",
      "Epoch121/400, Loss: 0.3630\n",
      "Epoch122/400, Loss: 0.3623\n",
      "Epoch123/400, Loss: 0.3670\n",
      "Epoch124/400, Loss: 0.3603\n",
      "Epoch125/400, Loss: 0.3582\n",
      "Epoch126/400, Loss: 0.3608\n",
      "Epoch127/400, Loss: 0.3556\n",
      "Epoch128/400, Loss: 0.3591\n",
      "Epoch129/400, Loss: 0.3561\n",
      "Epoch130/400, Loss: 0.3529\n",
      "Epoch131/400, Loss: 0.3527\n",
      "Epoch132/400, Loss: 0.3514\n",
      "Epoch133/400, Loss: 0.3507\n",
      "Epoch134/400, Loss: 0.3507\n",
      "Epoch135/400, Loss: 0.3464\n",
      "Epoch136/400, Loss: 0.3444\n",
      "Epoch137/400, Loss: 0.3434\n",
      "Epoch138/400, Loss: 0.3456\n",
      "Epoch139/400, Loss: 0.3447\n",
      "Epoch140/400, Loss: 0.3410\n",
      "Epoch141/400, Loss: 0.3420\n",
      "Epoch142/400, Loss: 0.3427\n",
      "Epoch143/400, Loss: 0.3400\n",
      "Epoch144/400, Loss: 0.3362\n",
      "Epoch145/400, Loss: 0.3354\n",
      "Epoch146/400, Loss: 0.3368\n",
      "Epoch147/400, Loss: 0.3339\n",
      "Epoch148/400, Loss: 0.3336\n",
      "Epoch149/400, Loss: 0.3302\n",
      "Epoch150/400, Loss: 0.3309\n",
      "Epoch151/400, Loss: 0.3298\n",
      "Epoch152/400, Loss: 0.3304\n",
      "Epoch153/400, Loss: 0.3289\n",
      "Epoch154/400, Loss: 0.3254\n",
      "Epoch155/400, Loss: 0.3269\n",
      "Epoch156/400, Loss: 0.3257\n",
      "Epoch157/400, Loss: 0.3251\n",
      "Epoch158/400, Loss: 0.3227\n",
      "Epoch159/400, Loss: 0.3229\n",
      "Epoch160/400, Loss: 0.3217\n",
      "Epoch161/400, Loss: 0.3158\n",
      "Epoch162/400, Loss: 0.3221\n",
      "Epoch163/400, Loss: 0.3173\n",
      "Epoch164/400, Loss: 0.3176\n",
      "Epoch165/400, Loss: 0.3163\n",
      "Epoch166/400, Loss: 0.3173\n",
      "Epoch167/400, Loss: 0.3162\n",
      "Epoch168/400, Loss: 0.3126\n",
      "Epoch169/400, Loss: 0.3126\n",
      "Epoch170/400, Loss: 0.3110\n",
      "Epoch171/400, Loss: 0.3087\n",
      "Epoch172/400, Loss: 0.3123\n",
      "Epoch173/400, Loss: 0.3114\n",
      "Epoch174/400, Loss: 0.3058\n",
      "Epoch175/400, Loss: 0.3083\n",
      "Epoch176/400, Loss: 0.3062\n",
      "Epoch177/400, Loss: 0.3045\n",
      "Epoch178/400, Loss: 0.3017\n",
      "Epoch179/400, Loss: 0.3079\n",
      "Epoch180/400, Loss: 0.3023\n",
      "Epoch181/400, Loss: 0.3029\n",
      "Epoch182/400, Loss: 0.3017\n",
      "Epoch183/400, Loss: 0.3016\n",
      "Epoch184/400, Loss: 0.3024\n",
      "Epoch185/400, Loss: 0.3000\n",
      "Epoch186/400, Loss: 0.2991\n",
      "Epoch187/400, Loss: 0.2997\n",
      "Epoch188/400, Loss: 0.2994\n",
      "Epoch189/400, Loss: 0.2906\n",
      "Epoch190/400, Loss: 0.2979\n",
      "Epoch191/400, Loss: 0.2961\n",
      "Epoch192/400, Loss: 0.2943\n",
      "Epoch193/400, Loss: 0.2943\n",
      "Epoch194/400, Loss: 0.2922\n",
      "Epoch195/400, Loss: 0.2937\n",
      "Epoch196/400, Loss: 0.2930\n",
      "Epoch197/400, Loss: 0.2909\n",
      "Epoch198/400, Loss: 0.2921\n",
      "Epoch199/400, Loss: 0.2862\n",
      "Epoch200/400, Loss: 0.2921\n",
      "Epoch201/400, Loss: 0.2892\n",
      "Accuracy: 0.9545\n",
      "Epoch202/400, Loss: 0.2880\n",
      "Epoch203/400, Loss: 0.2875\n",
      "Epoch204/400, Loss: 0.2879\n",
      "Epoch205/400, Loss: 0.2861\n",
      "Epoch206/400, Loss: 0.2872\n",
      "Epoch207/400, Loss: 0.2819\n",
      "Epoch208/400, Loss: 0.2860\n",
      "Epoch209/400, Loss: 0.2833\n",
      "Epoch210/400, Loss: 0.2827\n",
      "Epoch211/400, Loss: 0.2823\n",
      "Epoch212/400, Loss: 0.2773\n",
      "Epoch213/400, Loss: 0.2823\n",
      "Epoch214/400, Loss: 0.2809\n",
      "Epoch215/400, Loss: 0.2810\n",
      "Epoch216/400, Loss: 0.2788\n",
      "Epoch217/400, Loss: 0.2784\n",
      "Epoch218/400, Loss: 0.2793\n",
      "Epoch219/400, Loss: 0.2779\n",
      "Epoch220/400, Loss: 0.2757\n",
      "Epoch221/400, Loss: 0.2741\n",
      "Epoch222/400, Loss: 0.2728\n",
      "Epoch223/400, Loss: 0.2731\n",
      "Epoch224/400, Loss: 0.2739\n",
      "Epoch225/400, Loss: 0.2700\n",
      "Epoch226/400, Loss: 0.2737\n",
      "Epoch227/400, Loss: 0.2672\n",
      "Epoch228/400, Loss: 0.2731\n",
      "Epoch229/400, Loss: 0.2722\n",
      "Epoch230/400, Loss: 0.2717\n",
      "Epoch231/400, Loss: 0.2707\n",
      "Epoch232/400, Loss: 0.2691\n",
      "Epoch233/400, Loss: 0.2704\n",
      "Epoch234/400, Loss: 0.2657\n",
      "Epoch235/400, Loss: 0.2679\n",
      "Epoch236/400, Loss: 0.2673\n",
      "Epoch237/400, Loss: 0.2681\n",
      "Epoch238/400, Loss: 0.2674\n",
      "Epoch239/400, Loss: 0.2655\n",
      "Epoch240/400, Loss: 0.2662\n",
      "Epoch241/400, Loss: 0.2660\n",
      "Epoch242/400, Loss: 0.2643\n",
      "Epoch243/400, Loss: 0.2616\n",
      "Epoch244/400, Loss: 0.2618\n",
      "Epoch245/400, Loss: 0.2630\n",
      "Epoch246/400, Loss: 0.2614\n",
      "Epoch247/400, Loss: 0.2626\n",
      "Epoch248/400, Loss: 0.2625\n",
      "Epoch249/400, Loss: 0.2596\n",
      "Epoch250/400, Loss: 0.2600\n",
      "Epoch251/400, Loss: 0.2584\n",
      "Epoch252/400, Loss: 0.2583\n",
      "Epoch253/400, Loss: 0.2597\n",
      "Epoch254/400, Loss: 0.2581\n",
      "Epoch255/400, Loss: 0.2587\n",
      "Epoch256/400, Loss: 0.2582\n",
      "Epoch257/400, Loss: 0.2577\n",
      "Epoch258/400, Loss: 0.2568\n",
      "Epoch259/400, Loss: 0.2574\n",
      "Epoch260/400, Loss: 0.2553\n",
      "Epoch261/400, Loss: 0.2551\n",
      "Epoch262/400, Loss: 0.2555\n",
      "Epoch263/400, Loss: 0.2523\n",
      "Epoch264/400, Loss: 0.2552\n",
      "Epoch265/400, Loss: 0.2484\n",
      "Epoch266/400, Loss: 0.2543\n",
      "Epoch267/400, Loss: 0.2517\n",
      "Epoch268/400, Loss: 0.2499\n",
      "Epoch269/400, Loss: 0.2485\n",
      "Epoch270/400, Loss: 0.2504\n",
      "Epoch271/400, Loss: 0.2507\n",
      "Epoch272/400, Loss: 0.2512\n",
      "Epoch273/400, Loss: 0.2506\n",
      "Epoch274/400, Loss: 0.2478\n",
      "Epoch275/400, Loss: 0.2498\n",
      "Epoch276/400, Loss: 0.2481\n",
      "Epoch277/400, Loss: 0.2492\n",
      "Epoch278/400, Loss: 0.2449\n",
      "Epoch279/400, Loss: 0.2464\n",
      "Epoch280/400, Loss: 0.2463\n",
      "Epoch281/400, Loss: 0.2462\n",
      "Epoch282/400, Loss: 0.2423\n",
      "Epoch283/400, Loss: 0.2459\n",
      "Epoch284/400, Loss: 0.2462\n",
      "Epoch285/400, Loss: 0.2439\n",
      "Epoch286/400, Loss: 0.2407\n",
      "Epoch287/400, Loss: 0.2449\n",
      "Epoch288/400, Loss: 0.2416\n",
      "Epoch289/400, Loss: 0.2442\n",
      "Epoch290/400, Loss: 0.2421\n",
      "Epoch291/400, Loss: 0.2415\n",
      "Epoch292/400, Loss: 0.2401\n",
      "Epoch293/400, Loss: 0.2414\n",
      "Epoch294/400, Loss: 0.2417\n",
      "Epoch295/400, Loss: 0.2398\n",
      "Epoch296/400, Loss: 0.2410\n",
      "Epoch297/400, Loss: 0.2377\n",
      "Epoch298/400, Loss: 0.2396\n",
      "Epoch299/400, Loss: 0.2334\n",
      "Epoch300/400, Loss: 0.2406\n",
      "Epoch301/400, Loss: 0.2345\n",
      "Accuracy: 0.9545\n",
      "Epoch302/400, Loss: 0.2377\n",
      "Epoch303/400, Loss: 0.2386\n",
      "Epoch304/400, Loss: 0.2369\n",
      "Epoch305/400, Loss: 0.2369\n",
      "Epoch306/400, Loss: 0.2368\n",
      "Epoch307/400, Loss: 0.2363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch308/400, Loss: 0.2355\n",
      "Epoch309/400, Loss: 0.2351\n",
      "Epoch310/400, Loss: 0.2345\n",
      "Epoch311/400, Loss: 0.2326\n",
      "Epoch312/400, Loss: 0.2340\n",
      "Epoch313/400, Loss: 0.2329\n",
      "Epoch314/400, Loss: 0.2344\n",
      "Epoch315/400, Loss: 0.2282\n",
      "Epoch316/400, Loss: 0.2322\n",
      "Epoch317/400, Loss: 0.2311\n",
      "Epoch318/400, Loss: 0.2325\n",
      "Epoch319/400, Loss: 0.2310\n",
      "Epoch320/400, Loss: 0.2282\n",
      "Epoch321/400, Loss: 0.2329\n",
      "Epoch322/400, Loss: 0.2310\n",
      "Epoch323/400, Loss: 0.2297\n",
      "Epoch324/400, Loss: 0.2271\n",
      "Epoch325/400, Loss: 0.2288\n",
      "Epoch326/400, Loss: 0.2259\n",
      "Epoch327/400, Loss: 0.2254\n",
      "Epoch328/400, Loss: 0.2304\n",
      "Epoch329/400, Loss: 0.2289\n",
      "Epoch330/400, Loss: 0.2270\n",
      "Epoch331/400, Loss: 0.2265\n",
      "Epoch332/400, Loss: 0.2260\n",
      "Epoch333/400, Loss: 0.2273\n",
      "Epoch334/400, Loss: 0.2269\n",
      "Epoch335/400, Loss: 0.2242\n",
      "Epoch336/400, Loss: 0.2250\n",
      "Epoch337/400, Loss: 0.2259\n",
      "Epoch338/400, Loss: 0.2229\n",
      "Epoch339/400, Loss: 0.2263\n",
      "Epoch340/400, Loss: 0.2204\n",
      "Epoch341/400, Loss: 0.2257\n",
      "Epoch342/400, Loss: 0.2234\n",
      "Epoch343/400, Loss: 0.2223\n",
      "Epoch344/400, Loss: 0.2246\n",
      "Epoch345/400, Loss: 0.2213\n",
      "Epoch346/400, Loss: 0.2197\n",
      "Epoch347/400, Loss: 0.2234\n",
      "Epoch348/400, Loss: 0.2183\n",
      "Epoch349/400, Loss: 0.2239\n",
      "Epoch350/400, Loss: 0.2216\n",
      "Epoch351/400, Loss: 0.2207\n",
      "Epoch352/400, Loss: 0.2193\n",
      "Epoch353/400, Loss: 0.2208\n",
      "Epoch354/400, Loss: 0.2198\n",
      "Epoch355/400, Loss: 0.2203\n",
      "Epoch356/400, Loss: 0.2185\n",
      "Epoch357/400, Loss: 0.2179\n",
      "Epoch358/400, Loss: 0.2199\n",
      "Epoch359/400, Loss: 0.2182\n",
      "Epoch360/400, Loss: 0.2194\n",
      "Epoch361/400, Loss: 0.2188\n",
      "Epoch362/400, Loss: 0.2168\n",
      "Epoch363/400, Loss: 0.2181\n",
      "Epoch364/400, Loss: 0.2158\n",
      "Epoch365/400, Loss: 0.2177\n",
      "Epoch366/400, Loss: 0.2167\n",
      "Epoch367/400, Loss: 0.2171\n",
      "Epoch368/400, Loss: 0.2154\n",
      "Epoch369/400, Loss: 0.2153\n",
      "Epoch370/400, Loss: 0.2140\n",
      "Epoch371/400, Loss: 0.2146\n",
      "Epoch372/400, Loss: 0.2156\n",
      "Epoch373/400, Loss: 0.2142\n",
      "Epoch374/400, Loss: 0.2149\n",
      "Epoch375/400, Loss: 0.2117\n",
      "Epoch376/400, Loss: 0.2153\n",
      "Epoch377/400, Loss: 0.2136\n",
      "Epoch378/400, Loss: 0.2141\n",
      "Epoch379/400, Loss: 0.2140\n",
      "Epoch380/400, Loss: 0.2083\n",
      "Epoch381/400, Loss: 0.2135\n",
      "Epoch382/400, Loss: 0.2122\n",
      "Epoch383/400, Loss: 0.2132\n",
      "Epoch384/400, Loss: 0.2124\n",
      "Epoch385/400, Loss: 0.2114\n",
      "Epoch386/400, Loss: 0.2099\n",
      "Epoch387/400, Loss: 0.2094\n",
      "Epoch388/400, Loss: 0.2110\n",
      "Epoch389/400, Loss: 0.2108\n",
      "Epoch390/400, Loss: 0.2106\n",
      "Epoch391/400, Loss: 0.2095\n",
      "Epoch392/400, Loss: 0.2094\n",
      "Epoch393/400, Loss: 0.2087\n",
      "Epoch394/400, Loss: 0.2097\n",
      "Epoch395/400, Loss: 0.2097\n",
      "Epoch396/400, Loss: 0.2091\n",
      "Epoch397/400, Loss: 0.2068\n",
      "Epoch398/400, Loss: 0.2083\n",
      "Epoch399/400, Loss: 0.2076\n",
      "Epoch400/400, Loss: 0.2076\n",
      "Accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "losses = train(model, train_loader, test_loader, criterion, optimizer, num_epochs=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
